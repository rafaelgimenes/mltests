{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n",
    "<h2 align=\"center\"> Aula 07 - Extração de Informação (Parte 2)</h2>\n",
    "<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Extração de Relacionamentos</h2>\n",
    "<p>A extração de relacionamentos consiste em identificar a ligação entre diversas entidades nomeadas no texto. Isso envolve mencionar qual é o tipo da ligação entre duas entidades. Considere o exemplo de sentença abaixo.</p>\n",
    "\n",
    "<p>\"Carlos Alberto de Nogueira é o morador mais antigo da Rua Praça da Alegria.\"</p>\n",
    "\n",
    "<p>Temos as entidades:</p>\n",
    "\n",
    "* Carlos Alberto de Nogueira (PESSOA)\n",
    "* Rua Praça da Alegria (LOCALIDADE)\n",
    "\n",
    "<p>Essas mesmas entidades estão relacionadas da seguinte forma:</p>\n",
    "\n",
    "[Carlos Alberto de Nogueira (PERSON); morador mais antigo; Rua Praça da Alegria (LOCALIDADE)]\n",
    "\n",
    "\n",
    "<p>Um dos mais famosos exemplos de sistema de reconhecimento é o [Never-Ending Language Learning (NELL)](http://rtw.ml.cmu.edu/), projeto desenvolvido pela Universidade Carnigie Mellon, com participação do Google e inclusive de pesquisadores brasileiros financiados pelo CNPq. Esse projeto consiste em extrair relacionamentos de milhões de páginas da internet, criando uma gigantesca base de conhecimento.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Métodos para identificação de relacionamentos</h2>\n",
    "\n",
    "<p>Os métodos mais comuns para identificar relacionamentos entre entidades são:</p>\n",
    "\n",
    "* **Padrões codificados manualmente**: Basta criar padrões usando expressões regulares, por exemplo, para identificar que duas entidades se relacionam. Assim como em \"X mora em Y\" pode ser um padrão para identificar o relacionamento (X, mora_em, Y) entre uma entidade X do tipo PESSOA e uma entidade Y do tipo LOCALIDADE.\n",
    "* **Métodos bootstraping**: Com poucos dados, procura por ocorrências de duas entidades em que já se conhece o relacionamento (no Google, por exemplo), e usa os modelos encontrados como modelos para o mesmo relacionamento entre outras entidades.\n",
    "* **Métodos supervisionados**: Com base num corpus anotado com relacionamentos, criar modelos que 1) detecte quando existe o relacionamento entre duas entidades e 2) classifique o tipo de relacionamento entre elas. \n",
    "\n",
    "<p>Nesta aula, vamos ver um método supervisionado para classificar o relacionamento entre entidades, usando técnicas que já utilizamos em aulas anteriores.</p>\n",
    "\n",
    "<p>Para isso, utilizaremos alguns atributos mais comuns para o problema, como:</p>\n",
    "\n",
    "* Bag of Words/LSA\n",
    "* Flags indicadores dos tipos das entidades\n",
    "* Número de palavras entre as duas entidades\n",
    "* Flag indicando se o texto de uma entidade é composto pelo texto da outra\n",
    "* POS tags\n",
    "* etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Criando um Modelo Supervisionado</h2>\n",
    "<p> Vamos utilizar o corpus [Figure Eight: Medical Sentence Summary](https://www.kaggle.com/kmader/figure-eight-medical-sentence-summary), que possui diversas sentenças extraídas do PubMed, com entidades anotadas, assim como seus tipos de relacionamento.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_created_at</th>\n",
       "      <th>_canary</th>\n",
       "      <th>_id</th>\n",
       "      <th>_started_at</th>\n",
       "      <th>_channel</th>\n",
       "      <th>_trust</th>\n",
       "      <th>_worker_id</th>\n",
       "      <th>_country</th>\n",
       "      <th>_region</th>\n",
       "      <th>_city</th>\n",
       "      <th>_ip</th>\n",
       "      <th>direction</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>direction_gold</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>relation</th>\n",
       "      <th>relex_relcos</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>twrex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 13:48:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321892767</td>\n",
       "      <td>7/13/2014 13:48:14</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>27871219</td>\n",
       "      <td>NLD</td>\n",
       "      <td>07</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>87.210.207.223</td>\n",
       "      <td>IM CEFTRIAXONE treats URETHRAL OR RECTAL GONOR...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 13:51:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321894040</td>\n",
       "      <td>7/13/2014 13:51:07</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>17610000</td>\n",
       "      <td>GBR</td>\n",
       "      <td>I2</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>90.200.140.201</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA treats IM CEFTRIA...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 16:24:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321961909</td>\n",
       "      <td>7/13/2014 16:24:35</td>\n",
       "      <td>instagc</td>\n",
       "      <td>0.6639</td>\n",
       "      <td>25990856</td>\n",
       "      <td>USA</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>68.108.98.78</td>\n",
       "      <td>IM CEFTRIAXONE treats URETHRAL OR RECTAL GONOR...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 16:33:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321965723</td>\n",
       "      <td>7/13/2014 16:33:31</td>\n",
       "      <td>elite</td>\n",
       "      <td>0.3923</td>\n",
       "      <td>28276268</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>76.88.95.100</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA treats IM CEFTRIA...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 16:47:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321970904</td>\n",
       "      <td>7/13/2014 16:47:06</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>27597779</td>\n",
       "      <td>CAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>68.146.86.137</td>\n",
       "      <td>IM CEFTRIAXONE treats URETHRAL OR RECTAL GONOR...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 16:56:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321973849</td>\n",
       "      <td>7/13/2014 16:55:37</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.6639</td>\n",
       "      <td>28037714</td>\n",
       "      <td>GBR</td>\n",
       "      <td>I4</td>\n",
       "      <td>Mitcham</td>\n",
       "      <td>94.4.232.118</td>\n",
       "      <td>IM CEFTRIAXONE treats URETHRAL OR RECTAL GONOR...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>502808352</td>\n",
       "      <td>7/13/2014 17:14:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321979856</td>\n",
       "      <td>7/13/2014 17:14:06</td>\n",
       "      <td>prodege</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>2422962</td>\n",
       "      <td>USA</td>\n",
       "      <td>IA</td>\n",
       "      <td>Honey Creek</td>\n",
       "      <td>12.73.110.97</td>\n",
       "      <td>IM CEFTRIAXONE treats URETHRAL OR RECTAL GONOR...</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>142</td>\n",
       "      <td>treats</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>907845-FS1-2</td>\n",
       "      <td>For treatment of uncomplicated cervical, URETH...</td>\n",
       "      <td>URETHRAL OR RECTAL GONORRHEA</td>\n",
       "      <td>IM CEFTRIAXONE</td>\n",
       "      <td>RO-may_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 13:45:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321891302</td>\n",
       "      <td>7/13/2014 13:44:25</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>27871219</td>\n",
       "      <td>NLD</td>\n",
       "      <td>07</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>87.210.207.223</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 13:50:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321893871</td>\n",
       "      <td>7/13/2014 13:50:40</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>17610000</td>\n",
       "      <td>GBR</td>\n",
       "      <td>I2</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>90.200.140.201</td>\n",
       "      <td>OSTEOSARCOMA diagnosed by RETINOBLASTOMA</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 14:07:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321902037</td>\n",
       "      <td>7/13/2014 14:07:28</td>\n",
       "      <td>prodege</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>23977248</td>\n",
       "      <td>GBR</td>\n",
       "      <td>B5</td>\n",
       "      <td>Wembley</td>\n",
       "      <td>82.28.55.95</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 14:38:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321916349</td>\n",
       "      <td>7/13/2014 14:37:50</td>\n",
       "      <td>instagc</td>\n",
       "      <td>0.7431</td>\n",
       "      <td>15445601</td>\n",
       "      <td>USA</td>\n",
       "      <td>FL</td>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>24.129.68.254</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 16:14:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321958228</td>\n",
       "      <td>7/13/2014 16:13:26</td>\n",
       "      <td>instagc</td>\n",
       "      <td>0.7496</td>\n",
       "      <td>22391553</td>\n",
       "      <td>CAN</td>\n",
       "      <td>NB</td>\n",
       "      <td>Bathurst</td>\n",
       "      <td>156.34.43.239</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 16:41:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321968893</td>\n",
       "      <td>7/13/2014 16:41:08</td>\n",
       "      <td>instagc</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>24252915</td>\n",
       "      <td>GBR</td>\n",
       "      <td>B7</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>77.98.139.82</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>502808354</td>\n",
       "      <td>7/13/2014 16:46:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321970376</td>\n",
       "      <td>7/13/2014 16:45:49</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>27597779</td>\n",
       "      <td>CAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>68.146.86.137</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>217</td>\n",
       "      <td>diagnosed by</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>906321-FS1-13</td>\n",
       "      <td>Diagnosis specific malignancies available for ...</td>\n",
       "      <td>OSTEOSARCOMA</td>\n",
       "      <td>RETINOBLASTOMA</td>\n",
       "      <td>RO-has_manifestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>502808355</td>\n",
       "      <td>7/13/2014 13:51:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321894041</td>\n",
       "      <td>7/13/2014 13:51:07</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>17610000</td>\n",
       "      <td>GBR</td>\n",
       "      <td>I2</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>90.200.140.201</td>\n",
       "      <td>ALLERGY contraindicates NON ALLERGY</td>\n",
       "      <td>115</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>162</td>\n",
       "      <td>contraindicates</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>900413-FS1-10</td>\n",
       "      <td>Acute steady state moderate exercise significa...</td>\n",
       "      <td>ALLERGY</td>\n",
       "      <td>NON ALLERGY</td>\n",
       "      <td>RO-cause_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>502808355</td>\n",
       "      <td>7/13/2014 14:45:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321920117</td>\n",
       "      <td>7/13/2014 14:45:05</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>14861092</td>\n",
       "      <td>GBR</td>\n",
       "      <td>I2</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>90.194.137.76</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>115</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>162</td>\n",
       "      <td>contraindicates</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>900413-FS1-10</td>\n",
       "      <td>Acute steady state moderate exercise significa...</td>\n",
       "      <td>ALLERGY</td>\n",
       "      <td>NON ALLERGY</td>\n",
       "      <td>RO-cause_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>502808355</td>\n",
       "      <td>7/13/2014 15:02:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321929335</td>\n",
       "      <td>7/13/2014 15:01:45</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.6917</td>\n",
       "      <td>15189335</td>\n",
       "      <td>GBR</td>\n",
       "      <td>K3</td>\n",
       "      <td>Peterborough</td>\n",
       "      <td>81.156.166.189</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>115</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>162</td>\n",
       "      <td>contraindicates</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>900413-FS1-10</td>\n",
       "      <td>Acute steady state moderate exercise significa...</td>\n",
       "      <td>ALLERGY</td>\n",
       "      <td>NON ALLERGY</td>\n",
       "      <td>RO-cause_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>502808355</td>\n",
       "      <td>7/13/2014 15:33:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321942103</td>\n",
       "      <td>7/13/2014 15:32:56</td>\n",
       "      <td>prodege</td>\n",
       "      <td>0.6210</td>\n",
       "      <td>2143114</td>\n",
       "      <td>CAN</td>\n",
       "      <td>BC</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>24.84.160.12</td>\n",
       "      <td>ALLERGY contraindicates NON ALLERGY</td>\n",
       "      <td>115</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>162</td>\n",
       "      <td>contraindicates</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>900413-FS1-10</td>\n",
       "      <td>Acute steady state moderate exercise significa...</td>\n",
       "      <td>ALLERGY</td>\n",
       "      <td>NON ALLERGY</td>\n",
       "      <td>RO-cause_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>502808355</td>\n",
       "      <td>7/13/2014 15:58:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321952088</td>\n",
       "      <td>7/13/2014 15:58:31</td>\n",
       "      <td>prodege</td>\n",
       "      <td>0.6690</td>\n",
       "      <td>27096445</td>\n",
       "      <td>GBR</td>\n",
       "      <td>H9</td>\n",
       "      <td>London</td>\n",
       "      <td>86.186.168.254</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>115</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>162</td>\n",
       "      <td>contraindicates</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>900413-FS1-10</td>\n",
       "      <td>Acute steady state moderate exercise significa...</td>\n",
       "      <td>ALLERGY</td>\n",
       "      <td>NON ALLERGY</td>\n",
       "      <td>RO-cause_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>502808355</td>\n",
       "      <td>7/13/2014 16:01:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321952900</td>\n",
       "      <td>7/13/2014 16:00:21</td>\n",
       "      <td>prodege</td>\n",
       "      <td>0.5960</td>\n",
       "      <td>27934334</td>\n",
       "      <td>GBR</td>\n",
       "      <td>E4</td>\n",
       "      <td>Loughton</td>\n",
       "      <td>86.162.38.12</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>115</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>162</td>\n",
       "      <td>contraindicates</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>900413-FS1-10</td>\n",
       "      <td>Acute steady state moderate exercise significa...</td>\n",
       "      <td>ALLERGY</td>\n",
       "      <td>NON ALLERGY</td>\n",
       "      <td>RO-cause_of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id          ...                          twrex\n",
       "0   502808352          ...                   RO-may_treat\n",
       "1   502808352          ...                   RO-may_treat\n",
       "2   502808352          ...                   RO-may_treat\n",
       "3   502808352          ...                   RO-may_treat\n",
       "4   502808352          ...                   RO-may_treat\n",
       "5   502808352          ...                   RO-may_treat\n",
       "6   502808352          ...                   RO-may_treat\n",
       "7   502808354          ...           RO-has_manifestation\n",
       "8   502808354          ...           RO-has_manifestation\n",
       "9   502808354          ...           RO-has_manifestation\n",
       "10  502808354          ...           RO-has_manifestation\n",
       "11  502808354          ...           RO-has_manifestation\n",
       "12  502808354          ...           RO-has_manifestation\n",
       "13  502808354          ...           RO-has_manifestation\n",
       "14  502808355          ...                    RO-cause_of\n",
       "15  502808355          ...                    RO-cause_of\n",
       "16  502808355          ...                    RO-cause_of\n",
       "17  502808355          ...                    RO-cause_of\n",
       "18  502808355          ...                    RO-cause_of\n",
       "19  502808355          ...                    RO-cause_of\n",
       "\n",
       "[20 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('../input/figure-eight-medical-sentence-summary/train.csv')\n",
    "df_test = pd.read_csv('../input/figure-eight-medical-sentence-summary/test.csv')\n",
    "\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['treats', 'diagnosed by', 'contraindicates', 'causes', 'location',\n",
       "       'is location of', 'location of', 'is diagnosed by',\n",
       "       'diagnose_by_test_or_drug'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['relation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Transformamos as sentenças e tipos de relacionamento em matrizes numpy. Também binarizamos os rótulos dos relacionamentos, para utilizarmos no nosso classificador logo mais.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " 'For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " 'For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " 'For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " 'For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " 'For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " 'For treatment of uncomplicated cervical, URETHRAL OR RECTAL GONORRHEA CDC and others recommend IM ceftriaxone or oral cefixime; IM CEFTRIAXONE is drug of choice for pharyngeal infections.'\n",
      " \"Diagnosis specific malignancies available for evaluation included ALL, acute myeloid leukaemia (AML), Hodgkin's disease, NHL, rhabdomyosarcoma, neuroblastoma, retinoblastoma, OSTEOSARCOMA Wilms' tumour, RETINOBLASTOMA Ewings' sarcoma, central nervous system (CNS) tumours and hepatoblastoma.\"\n",
      " \"Diagnosis specific malignancies available for evaluation included ALL, acute myeloid leukaemia (AML), Hodgkin's disease, NHL, rhabdomyosarcoma, neuroblastoma, retinoblastoma, OSTEOSARCOMA Wilms' tumour, RETINOBLASTOMA Ewings' sarcoma, central nervous system (CNS) tumours and hepatoblastoma.\"\n",
      " \"Diagnosis specific malignancies available for evaluation included ALL, acute myeloid leukaemia (AML), Hodgkin's disease, NHL, rhabdomyosarcoma, neuroblastoma, retinoblastoma, OSTEOSARCOMA Wilms' tumour, RETINOBLASTOMA Ewings' sarcoma, central nervous system (CNS) tumours and hepatoblastoma.\"]\n",
      "[[1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = df_train['sentence'].as_matrix()\n",
    "y_train = df_train['relation'].as_matrix()\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_train = label_binarize(y_train, classes=df_train['relation'].unique())\n",
    "\n",
    "print(x_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Como não temos os tipos das entidades, mas sabemos que se trata de nomes de medicamentos e doenças na maioria dos casos, não utilizaremos o tipo das entidades como atributos, mas utilizaremos os POS tags de todas as palavras entre as entidades. Vamos criar outras matrizes com esses atributos. </p>\n",
    "\n",
    "<p>Para os POS Tags, vamos fazer algo parecido ao chunking sugerido em https://courses.cs.washington.edu/courses/cse517/13wi/slides/cse517wi13-RelationExtraction.pdf, mas ao invés de usar chunking, vamos criar 3-grams desses POS tags para simplificar.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " ' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " ' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " ' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " ' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " ' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " ' CDC and others recommend IM ceftriaxone or oral cefixime; '\n",
      " \" Wilms' tumour, \" \" Wilms' tumour, \" \" Wilms' tumour, \"]\n"
     ]
    }
   ],
   "source": [
    "x_train_sub_list = []\n",
    "\n",
    "for i, row in df_train.iterrows():\n",
    "    pos_t1 = row['sentence'].find(row['term1'])\n",
    "    len_t1 = len(row['term1'])\n",
    "    \n",
    "    pos_t2 = row['sentence'].find(row['term2'])    \n",
    "    \n",
    "    x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n",
    "    \n",
    "\n",
    "x_train_sub = np.array(x_train_sub_list)\n",
    "\n",
    "print(x_train_sub[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Agora vamos definir duas funções de tokenização: uma para tokenizar bag-of-words e outra para tokenizar os POS tags</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'CC', 'NNS', 'VBP', 'NNP', 'NN', 'CC', 'JJ', 'NN', ':']\n",
      "['NNP', 'POS', 'NN', ',']\n",
      "['NNP', 'POS', 'NN', ',']\n",
      "['NNP', 'POS', 'NN', ',']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def my_tokenizer_pos(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    return [pos[1] for pos in pos_tags]\n",
    "\n",
    "# testando nossa função:\n",
    "\n",
    "for x in x_train_sub[:10]:\n",
    "    print(my_tokenizer_pos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer_bow(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n",
    "    \n",
    "    lemmas = []\n",
    "    for w in non_punctuation:\n",
    "        if w[1].startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif w[1].startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif w[1].startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif w[1].startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = wordnet.NOUN\n",
    "        \n",
    "        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Vamos reaproveitar a classe para seleção de atributos usando SVD.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class SVDDimSelect(object):\n",
    "    def fit(self, X, y=None):        \n",
    "        try:\n",
    "            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n",
    "            self.svd_transformer.fit(X)\n",
    "        \n",
    "            cummulative_variance = 0.0\n",
    "            k = 0\n",
    "            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n",
    "                cummulative_variance += var\n",
    "                if cummulative_variance >= 0.5:\n",
    "                    break\n",
    "                else:\n",
    "                    k += 1\n",
    "                \n",
    "            self.svd_transformer = TruncatedSVD(n_components=k)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return self.svd_transformer.fit(X)\n",
    "    \n",
    "    def transform(self, X, Y=None):\n",
    "        return self.svd_transformer.transform(X)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Agora vamos criar nosso Pipeline. Em resumo, vamos usar o TFIDF Vectorizer e o nosso POS Tagger em paralelo, e depois juntar os atributos para redimensionar usando o SVD.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))\n",
    "\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "                        ('union', FeatureUnion([('bow', TfidfVectorizer(tokenizer=my_tokenizer_bow)),\\\n",
    "                                                ('pos', Pipeline([('pos-vect', CountVectorizer(tokenizer=my_tokenizer_pos)), \\\n",
    "                                                         ('pos-tfidf', TfidfTransformer())]))\n",
    "                                               ])),\\\n",
    "                       ('svd', SVDDimSelect()), \\\n",
    "                       ('clf', clf)])\n",
    "\n",
    "par = {'clf__estimator__C' : np.logspace(-4, 4, 20)}\n",
    "\n",
    "hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='f1_weighted', n_jobs=1, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Agora vamos treinar os algoritmos</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13340,)\n",
      "(13340, 9)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_sub.shape)\n",
    "print(y_train.shape)\n",
    "#só pra rodar mais rapido meu exercicio\n",
    "#hyperpar_selector.fit(X=x_train_sub, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_test = df_test['sentence'].as_matrix()\n",
    "y_test = df_test['relation'].as_matrix()\n",
    "\n",
    "y_test = label_binarize(y_test, classes=df_train['relation'].unique())\n",
    "\n",
    "x_test_sub_list = []\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    pos_t1 = row['sentence'].find(row['term1'])\n",
    "    pos_t2 = row['sentence'].find(row['term2'])    \n",
    "\n",
    "    if pos_t1 < pos_t2:\n",
    "        len_t1 = len(row['term1'])    \n",
    "        x_test_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n",
    "    else:\n",
    "        len_t2 = len(row['term2'])    \n",
    "        x_test_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n",
    "    \n",
    "\n",
    "x_test_sub = np.array(x_test_sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predicted = hyperpar_selector.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#só pra comitar mais rapido meu exerciicio\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#print(classification_report(y_test, y_predicted, target_names=df_train['relation'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Exercício 7:</b> Treine um modelo de extração de relacionamentos em Português, utilizando o corpus extraído do DBPedia e com relacionamentos entre pares de entidades anotadas.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz#egg=pt_core_news_sm==2.1.0\r\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz (12.8MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 71.8MB/s \r\n",
      "\u001b[?25hInstalling collected packages: pt-core-news-sm\r\n",
      "  Running setup.py install for pt-core-news-sm ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hSuccessfully installed pt-core-news-sm-2.1.0\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pt_core_news_sm\n",
    "nlp = pt_core_news_sm.load()\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = False\n",
    "lista = []\n",
    "with open('../input/dbpedia-with-entity-relations-in-portuguese/DBpediaRelations-PT-0.2.txt') as file:\n",
    "    for line in file:\n",
    "        if(line.strip() == '*********'):\n",
    "            break\n",
    "        if(line.strip() == '****************************'):\n",
    "            if (not start):\n",
    "                dic = {}\n",
    "            else:\n",
    "                #print(dic)\n",
    "                lista.append(dic)\n",
    "            start = ~start\n",
    "        else:\n",
    "            if (start):\n",
    "                if (line.strip() != ''):\n",
    "                    #print(line)\n",
    "                    aux = line.split(':')\n",
    "                    if(len(aux) < 2):\n",
    "                        print(aux)\n",
    "                    dic[aux[0].strip()] = aux[1].strip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "porcentagem = 0.7\n",
    "mask = np.ones(int(len(lista)*porcentagem))\n",
    "mask = np.concatenate((mask, np.zeros(len(lista) - len(mask)))) == 1\n",
    "random.shuffle(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = pd.DataFrame(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = lista[mask]\n",
    "teste = lista[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTITY1</th>\n",
       "      <th>ENTITY2</th>\n",
       "      <th>MANUALLY CHECKED</th>\n",
       "      <th>REL TYPE</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>TYPE1</th>\n",
       "      <th>TYPE2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>América Latina</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>locatedInArea</td>\n",
       "      <td>A América Latina destaca-se ainda por sua prod...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>Ulm</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>origin</td>\n",
       "      <td>É em Ulm que nasce Albert Einstein, em 14 de m...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Afeganistão</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>A capital do Afeganistão foi transferida em 17...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Afeganistão</td>\n",
       "      <td>Hamid Karzai</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>keyPerson</td>\n",
       "      <td>Em dezembro de 2001 o Conselho de Segurança da...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>O nome Angola é uma derivação portuguesa do te...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>No dia 11 de novembro de 1975 foi proclamada a...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>Em Angola, e mais especialmente em Luanda, a e...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Amazonas</td>\n",
       "      <td>Manaus</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>Ganhou a condição de Província do Amazonas pel...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Amazonas</td>\n",
       "      <td>Maués</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>Chegaram ao Amazonas os primeiros imigrantes, ...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Amazonas</td>\n",
       "      <td>Manaus</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>O Amazonas possui várias instituições educacio...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Alagoas</td>\n",
       "      <td>Maceió</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>locatedInArea</td>\n",
       "      <td>A Lei Provincial de 9 de dezembro de 1839 tran...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Albânia</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>locatedInArea</td>\n",
       "      <td>A Albânia é um país europeu situado na penínsu...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ENTITY1       ENTITY2    ...        TYPE1     TYPE2\n",
       "2    América Latina        Brasil    ...     LOCATION  LOCATION\n",
       "5   Albert Einstein           Ulm    ...       PERSON  LOCATION\n",
       "14      Afeganistão      Kandahar    ...     LOCATION  LOCATION\n",
       "15      Afeganistão  Hamid Karzai    ...     LOCATION    PERSON\n",
       "17           Angola        Luanda    ...     LOCATION  LOCATION\n",
       "18           Angola        Luanda    ...     LOCATION  LOCATION\n",
       "19           Angola        Luanda    ...     LOCATION  LOCATION\n",
       "23         Amazonas        Manaus    ...     LOCATION  LOCATION\n",
       "27         Amazonas         Maués    ...     LOCATION  LOCATION\n",
       "29         Amazonas        Manaus    ...     LOCATION  LOCATION\n",
       "31          Alagoas        Maceió    ...     LOCATION  LOCATION\n",
       "32          Albânia        Tirana    ...     LOCATION  LOCATION\n",
       "\n",
       "[12 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTITY1</th>\n",
       "      <th>ENTITY2</th>\n",
       "      <th>MANUALLY CHECKED</th>\n",
       "      <th>REL TYPE</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>TYPE1</th>\n",
       "      <th>TYPE2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>América Latina</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>locatedInArea</td>\n",
       "      <td>A América Latina localiza-se totalmente no hem...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>América Latina</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>locatedInArea</td>\n",
       "      <td>A América Latina não apresenta, ao contrário d...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>Ulm</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>origin</td>\n",
       "      <td>Albert Einstein nasceu na região alemã de Würt...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>Württemberg</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>origin</td>\n",
       "      <td>Albert Einstein nasceu na região alemã de Würt...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adriano</td>\n",
       "      <td>Trajano</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>successor</td>\n",
       "      <td>Nascido em Itálica na atual Espanha, ou em Rom...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adriano</td>\n",
       "      <td>Itália</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>origin</td>\n",
       "      <td>Nascido em Itálica na atual Espanha, ou em Rom...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adriano</td>\n",
       "      <td>Trajano</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>successor</td>\n",
       "      <td>Talvez por entender que o império esgotara sua...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Adriano</td>\n",
       "      <td>Trajano</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>successor</td>\n",
       "      <td>Adriano também retificou os limites de uma out...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dom Afonso de Portugal</td>\n",
       "      <td>Lisboa</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>origin</td>\n",
       "      <td>Dom Afonso de Portugal (Lisboa, 18 de Maio de ...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dom Afonso de Portugal</td>\n",
       "      <td>Santarém</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>deathOrBurialPlace</td>\n",
       "      <td>Dom Afonso de Portugal (Lisboa, 18 de Maio de ...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Afonso</td>\n",
       "      <td>Alfange</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>deathOrBurialPlace</td>\n",
       "      <td>Afonso morreu em circunstâncias misteriosas, d...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Afeganistão</td>\n",
       "      <td>Cabul</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>other</td>\n",
       "      <td>A capital do Afeganistão foi transferida em 17...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ENTITY1      ENTITY2    ...        TYPE1     TYPE2\n",
       "0           América Latina    Argentina    ...     LOCATION  LOCATION\n",
       "1           América Latina       Brasil    ...     LOCATION  LOCATION\n",
       "3          Albert Einstein          Ulm    ...       PERSON  LOCATION\n",
       "4          Albert Einstein  Württemberg    ...       PERSON  LOCATION\n",
       "6                  Adriano      Trajano    ...       PERSON    PERSON\n",
       "7                  Adriano       Itália    ...       PERSON  LOCATION\n",
       "8                  Adriano      Trajano    ...       PERSON    PERSON\n",
       "9                  Adriano      Trajano    ...       PERSON    PERSON\n",
       "10  Dom Afonso de Portugal       Lisboa    ...       PERSON  LOCATION\n",
       "11  Dom Afonso de Portugal     Santarém    ...       PERSON  LOCATION\n",
       "12                  Afonso      Alfange    ...       PERSON  LOCATION\n",
       "13             Afeganistão        Cabul    ...     LOCATION  LOCATION\n",
       "\n",
       "[12 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98023"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tamanho\n",
    "len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A América Latina localiza-se totalmente no hemisfério ocidental, sendo atravessada pelo Trópico de Câncer, que corta a parte central do México; pelo Equador, que passa pelo Brasil, Colômbia, Equador e toca o norte do Peru; e pelo Trópico de Capricórnio, que atravessa o Brasil, o Paraguai, a Argentina e o Chile.'\n",
      " 'A América Latina não apresenta, ao contrário da América do Norte, grandes extensões lacustres, mas ainda assim possui inúmeras lagoas costeiras, sobretudo na vertente atlântica, como a lagoa dos Patos, no Brasil; lagoas de inundação nas planícies Amazônica e do Orinoco; e lagos de altitude, como o Titicaca, entre o Peru e a Bolívia.'\n",
      " 'Albert Einstein nasceu na região alemã de Württemberg, na cidade de Ulm, numa família judaica.'\n",
      " 'Albert Einstein nasceu na região alemã de Württemberg, na cidade de Ulm, numa família judaica.'\n",
      " 'Nascido em Itálica na atual Espanha, ou em Roma, na Itália, Adriano era descendente de colonos romanos domiciliados no Sul da Hispânia e primo de Trajano, tendo sido nomeado por este para uma série de dignidades públicas que o fizeram aparecer como herdeiro presuntivo deste imperador.'\n",
      " 'Nascido em Itálica na atual Espanha, ou em Roma, na Itália, Adriano era descendente de colonos romanos domiciliados no Sul da Hispânia e primo de Trajano, tendo sido nomeado por este para uma série de dignidades públicas que o fizeram aparecer como herdeiro presuntivo deste imperador.'\n",
      " 'Talvez por entender que o império esgotara sua capacidade de expansão, Adriano abandonou a política de conquistas de Trajano, adotando outra nitidamente defensiva, optando pela via diplomática para resolver questões relativas ao relacionamento com povos vizinhos.'\n",
      " 'Adriano também retificou os limites de uma outra conquista de Trajano, esta já antiga, a Dácia (atual Roménia), cedendo aos sármatas a planície do Baixo Danúbio e concentrando a ocupação romana na região da Transilvânia, protegida pela barreira natural dos Cárpatos.'\n",
      " 'Dom Afonso de Portugal (Lisboa, 18 de Maio de 1475 – Santarém, 13 de Julho de 1491) era filho único.'\n",
      " 'Dom Afonso de Portugal (Lisboa, 18 de Maio de 1475 – Santarém, 13 de Julho de 1491) era filho único.'\n",
      " 'Afonso morreu em circunstâncias misteriosas, de uma queda de cavalo durante um passeio, em Alfange, Santarém, à beira do Tejo.'\n",
      " 'A capital do Afeganistão foi transferida em 1776 de Kandahar para Cabul e parte do Império Afegão foi cedida aos impérios vizinhos em 1893.']\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "treino['REL TYPE'].unique()\n",
    "\n",
    "x_train = treino['SENTENCE'].as_matrix()\n",
    "y_train = treino['REL TYPE'].as_matrix()\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_train = label_binarize(y_train, classes=treino['REL TYPE'].unique())\n",
    "\n",
    "print(x_train[:12])\n",
    "print(y_train[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' localiza-se totalmente no hemisfério ocidental, sendo atravessada pelo Trópico de Câncer, que corta a parte central do México; pelo Equador, que passa pelo Brasil, Colômbia, Equador e toca o norte do Peru; e pelo Trópico de Capricórnio, que atravessa o Brasil, o Paraguai, a '\n",
      " ' não apresenta, ao contrário da América do Norte, grandes extensões lacustres, mas ainda assim possui inúmeras lagoas costeiras, sobretudo na vertente atlântica, como a lagoa dos Patos, no '\n",
      " ' nasceu na região alemã de Württemberg, na cidade de '\n",
      " ' nasceu na região alemã de '\n",
      " ' era descendente de colonos romanos domiciliados no Sul da Hispânia e primo de '\n",
      " ', ' ' abandonou a política de conquistas de '\n",
      " ' também retificou os limites de uma outra conquista de ' ' ('\n",
      " ' (Lisboa, 18 de Maio de 1475 – '\n",
      " ' morreu em circunstâncias misteriosas, de uma queda de cavalo durante um passeio, em '\n",
      " ' foi transferida em 1776 de Kandahar para ']\n"
     ]
    }
   ],
   "source": [
    "x_train_sub_list = []\n",
    "\n",
    "for i, row in treino.iterrows():\n",
    "    pos_t1 = row['SENTENCE'].find(row['ENTITY1'])\n",
    "    pos_t2 = row['SENTENCE'].find(row['ENTITY2'])    \n",
    "    \n",
    "    if pos_t1 < pos_t2:\n",
    "        len_t1 = len(row['ENTITY1'])\n",
    "        x_train_sub_list.append(row['SENTENCE'][pos_t1+len_t1:pos_t2])\n",
    "    else:\n",
    "        len_t2 = len(row['ENTITY2'])\n",
    "        x_train_sub_list.append(row['SENTENCE'][pos_t2+len_t2:pos_t1])\n",
    "        \n",
    "x_train_sub = np.array(x_train_sub_list)\n",
    "\n",
    "print(x_train_sub[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JJ', 'NN', 'DT', 'NN', 'NN', ',', 'VBP', 'JJ', 'NN', 'NNP', 'FW', 'NNP', ',', 'NN', 'VBD', 'DT', 'JJ', 'JJ', 'VBP', 'NNP', ':', 'NN', 'NNP', ',', 'NN', 'NN', 'NN', 'NNP', ',', 'NNP', ',', 'NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'VBP', 'NNP', ':', 'CC', 'VB', 'NNP', 'FW', 'NNP', ',', 'NN', 'NN', 'NN', 'NNP', ',', 'NN', 'NNP', ',', 'DT']\n",
      "['JJ', 'NN', ',', 'VBP', 'NN', 'NN', 'NNP', 'VBP', 'NNP', ',', 'VBZ', 'JJ', 'NNS', ',', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'NNS', ',', 'NN', 'TO', 'NN', 'NN', ',', 'VB', 'DT', 'JJ', 'NN', 'NNP', ',', 'DT']\n",
      "['RB', 'JJ', 'NN', 'NN', 'IN', 'NNP', ',', 'RB', 'NN', 'IN']\n",
      "['RB', 'JJ', 'NN', 'NN', 'IN']\n",
      "['NN', 'NN', 'IN', 'FW', 'NNS', 'VBP', 'DT', 'NNP', 'VBZ', 'NNP', 'NN', 'NN', 'IN']\n",
      "[',']\n",
      "['VB', 'DT', 'NN', 'IN', 'FW', 'FW']\n",
      "['NN', 'NN', 'JJ', 'VBZ', 'FW', 'FW', 'JJ', 'NN', 'IN']\n",
      "['(']\n",
      "['(', 'NNP', ',', 'CD', 'FW', 'NNP', 'IN', 'CD', 'NN']\n",
      "['NN', 'NN', 'NN', 'NN', ',', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'JJ', 'NN', ',', 'NN']\n",
      "['NN', 'NN', 'NN', 'CD', 'IN', 'NNP', 'NN']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def my_tokenizer_pos(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    return [pos[1] for pos in pos_tags]\n",
    "\n",
    "# testando nossa função:\n",
    "\n",
    "for x in x_train_sub[:12]:\n",
    "    print(my_tokenizer_pos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('portuguese')\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer_bow(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    non_stopwords = [w for w in words if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n",
    "    \n",
    "    lemmas = []\n",
    "    for w in non_punctuation:\n",
    "                \n",
    "        lemmas.append(stemmer.stem(w))\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class SVDDimSelect(object):\n",
    "    def fit(self, X, y=None):        \n",
    "        try:\n",
    "            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n",
    "            self.svd_transformer.fit(X)\n",
    "            cummulative_variance = 0.0\n",
    "            k = 0\n",
    "            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n",
    "                cummulative_variance += var\n",
    "                if cummulative_variance >= 0.5:\n",
    "                    break\n",
    "                else:\n",
    "                    k += 1\n",
    "            self.svd_transformer = TruncatedSVD(n_components=k)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return self.svd_transformer.fit(X)\n",
    "    \n",
    "    def transform(self, X, Y=None):\n",
    "        return self.svd_transformer.transform(X)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))\n",
    "\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "                        ('union', FeatureUnion([('bow', TfidfVectorizer(tokenizer=my_tokenizer_bow)),\\\n",
    "                                                ('pos', Pipeline([('pos-vect', CountVectorizer(tokenizer=my_tokenizer_pos)), \\\n",
    "                                                         ('pos-tfidf', TfidfTransformer())]))\n",
    "                                               ])),\\\n",
    "                       ('svd', SVDDimSelect()), \\\n",
    "                       ('clf', clf)])\n",
    "\n",
    "par = {'clf__estimator__C' : np.logspace(-4, 4, 20)}\n",
    "\n",
    "hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='f1_weighted', n_jobs=1, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68616,)\n",
      "(68616, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_sub.shape)\n",
    "print(y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('bow', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_r...tate=0, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None))]),\n",
       "          fit_params=None, iid='warn', n_iter=20, n_jobs=1,\n",
       "          param_distributions={'clf__estimator__C': array([1.00000e-04, 2.63665e-04, 6.95193e-04, 1.83298e-03, 4.83293e-03,\n",
       "       1.27427e-02, 3.35982e-02, 8.85867e-02, 2.33572e-01, 6.15848e-01,\n",
       "       1.62378e+00, 4.28133e+00, 1.12884e+01, 2.97635e+01, 7.84760e+01,\n",
       "       2.06914e+02, 5.45559e+02, 1.43845e+03, 3.79269e+03, 1.00000e+04])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperpar_selector.fit(X=x_train_sub[:100], y=y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = teste['SENTENCE'].as_matrix()\n",
    "y_test = teste['REL TYPE'].as_matrix()\n",
    "\n",
    "y_test = label_binarize(y_test, classes=treino['REL TYPE'].unique())\n",
    "\n",
    "x_test_sub_list = []\n",
    "\n",
    "for i, row in teste.iterrows():\n",
    "    pos_t1 = row['SENTENCE'].find(row['ENTITY1'])\n",
    "    pos_t2 = row['SENTENCE'].find(row['ENTITY2'])    \n",
    "\n",
    "    if pos_t1 < pos_t2:\n",
    "        len_t1 = len(row['ENTITY1'])    \n",
    "        x_test_sub_list.append(row['SENTENCE'][pos_t1+len_t1:pos_t2])\n",
    "    else:\n",
    "        len_t2 = len(row['ENTITY2'])    \n",
    "        x_test_sub_list.append(row['SENTENCE'][pos_t2+len_t2:pos_t1])\n",
    "    \n",
    "\n",
    "x_test_sub = np.array(x_test_sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = hyperpar_selector.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     locatedInArea       0.86      0.40      0.55     14047\n",
      "            origin       0.17      0.00      0.01      7881\n",
      "         successor       0.00      0.00      0.00       160\n",
      "deathOrBurialPlace       0.40      0.58      0.47      2157\n",
      "             other       0.40      0.12      0.19      3165\n",
      "         keyPerson       0.00      0.00      0.00       116\n",
      "            partOf       0.00      0.00      0.00      1663\n",
      "      influencedBy       0.00      0.00      0.00        51\n",
      "           partner       0.00      0.00      0.00        65\n",
      "            parent       0.00      0.00      0.00       102\n",
      "\n",
      "         micro avg       0.67      0.25      0.36     29407\n",
      "         macro avg       0.18      0.11      0.12     29407\n",
      "      weighted avg       0.53      0.25      0.32     29407\n",
      "       samples avg       0.25      0.25      0.25     29407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predicted, target_names=treino['REL TYPE'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
