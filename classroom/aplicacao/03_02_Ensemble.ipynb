{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comitês de máquinas (Ensemble)\n",
    "\n",
    "Os comitês de máquinas são a combinação de um mais classificadores / regressores para chegar a um rótulo. Nesse sentido, podem ser empregados classificadores que foram treinados com diferentes conjuntos de dados, ou poderão ser empregados classificadores que foram treinados no mesmo conjunto de dados utilizando estratégias de aprendizado diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de KNeighbors simples:', scores.mean())\n",
    "\n",
    "model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de KNeighbors Bagging (c/ 10 estimators):', scores.mean())\n",
    "\n",
    "model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, n_estimators=20)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de KNeighbors Bagging (c/ 20 estimators):', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Acurácia de Decision Tree puro:', scores.mean())\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Acurácia de Random Forest:', scores.mean())\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Acurácia de Extreme Randomized Trees:', scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.3, max_depth=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de Gradient Boosting Tree:', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Aumentar o número de estimadores de forma sistemática no BaggingClassifier e analisar quando o resultado para de subir.\n",
    "2. Observar quando o desempenho para de aumentar ao elevar o número de estimadores no RandomForest e ExtraTree.\n",
    "3. Incluir o KNN no VotingClassifier e modificar o código para testar tanto o KNN isolado como o resultado do VotingClassifier com esse estimador a mais.\n",
    "4. Teste valores maiores e menores de learning_rate e max_depth. O que pode ser observado?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
