{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n<h2 align=\"center\"> Aula 07 - Extração de Informação (Parte 2)</h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>"},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Extração de Relacionamentos</h2>\n<p>A extração de relacionamentos consiste em identificar a ligação entre diversas entidades nomeadas no texto. Isso envolve mencionar qual é o tipo da ligação entre duas entidades. Considere o exemplo de sentença abaixo.</p>\n\n<p>\"Carlos Alberto de Nogueira é o morador mais antigo da Rua Praça da Alegria.\"</p>\n\n<p>Temos as entidades:</p>\n\n* Carlos Alberto de Nogueira (PESSOA)\n* Rua Praça da Alegria (LOCALIDADE)\n\n<p>Essas mesmas entidades estão relacionadas da seguinte forma:</p>\n\n[Carlos Alberto de Nogueira (PERSON); morador mais antigo; Rua Praça da Alegria (LOCALIDADE)]\n\n\n<p>Um dos mais famosos exemplos de sistema de reconhecimento é o [Never-Ending Language Learning (NELL)](http://rtw.ml.cmu.edu/), projeto desenvolvido pela Universidade Carnigie Mellon, com participação do Google e inclusive de pesquisadores brasileiros financiados pelo CNPq. Esse projeto consiste em extrair relacionamentos de milhões de páginas da internet, criando uma gigantesca base de conhecimento.</p>"},{"metadata":{},"cell_type":"markdown","source":"<h2>2. Métodos para identificação de relacionamentos</h2>\n\n<p>Os métodos mais comuns para identificar relacionamentos entre entidades são:</p>\n\n* **Padrões codificados manualmente**: Basta criar padrões usando expressões regulares, por exemplo, para identificar que duas entidades se relacionam. Assim como em \"X mora em Y\" pode ser um padrão para identificar o relacionamento (X, mora_em, Y) entre uma entidade X do tipo PESSOA e uma entidade Y do tipo LOCALIDADE.\n* **Métodos bootstraping**: Com poucos dados, procura por ocorrências de duas entidades em que já se conhece o relacionamento (no Google, por exemplo), e usa os modelos encontrados como modelos para o mesmo relacionamento entre outras entidades.\n* **Métodos supervisionados**: Com base num corpus anotado com relacionamentos, criar modelos que 1) detecte quando existe o relacionamento entre duas entidades e 2) classifique o tipo de relacionamento entre elas. \n\n<p>Nesta aula, vamos ver um método supervisionado para classificar o relacionamento entre entidades, usando técnicas que já utilizamos em aulas anteriores.</p>\n\n<p>Para isso, utilizaremos alguns atributos mais comuns para o problema, como:</p>\n\n* Bag of Words/LSA\n* Flags indicadores dos tipos das entidades\n* Número de palavras entre as duas entidades\n* Flag indicando se o texto de uma entidade é composto pelo texto da outra\n* POS tags\n* etc\n\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>3. Criando um Modelo Supervisionado</h2>\n<p> Vamos utilizar o corpus [Figure Eight: Medical Sentence Summary](https://www.kaggle.com/kmader/figure-eight-medical-sentence-summary), que possui diversas sentenças extraídas do PubMed, com entidades anotadas, assim como seus tipos de relacionamento.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('../input/figure-eight-medical-sentence-summary/train.csv')\ndf_test = pd.read_csv('../input/figure-eight-medical-sentence-summary/test.csv')\n\ndf_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['relation'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Transformamos as sentenças e tipos de relacionamento em matrizes numpy. Também binarizamos os rótulos dos relacionamentos, para utilizarmos no nosso classificador logo mais.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nx_train = df_train['sentence'].as_matrix()\ny_train = df_train['relation'].as_matrix()\n\nfrom sklearn.preprocessing import label_binarize\n\ny_train = label_binarize(y_train, classes=df_train['relation'].unique())\n\nprint(x_train[:10])\nprint(y_train[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Como não temos os tipos das entidades, mas sabemos que se trata de nomes de medicamentos e doenças na maioria dos casos, não utilizaremos o tipo das entidades como atributos, mas utilizaremos os POS tags de todas as palavras entre as entidades. Vamos criar outras matrizes com esses atributos. </p>\n\n<p>Para os POS Tags, vamos fazer algo parecido ao chunking sugerido em https://courses.cs.washington.edu/courses/cse517/13wi/slides/cse517wi13-RelationExtraction.pdf, mas ao invés de usar chunking, vamos criar 3-grams desses POS tags para simplificar.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_sub_list = []\n\nfor i, row in df_train.iterrows():\n    pos_t1 = row['sentence'].find(row['term1'])\n    len_t1 = len(row['term1'])\n    \n    pos_t2 = row['sentence'].find(row['term2'])    \n    \n    x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    \n\nx_train_sub = np.array(x_train_sub_list)\n\nprint(x_train_sub[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos definir duas funções de tokenização: uma para tokenizar bag-of-words e outra para tokenizar os POS tags</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\ndef my_tokenizer_pos(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    return [pos[1] for pos in pos_tags]\n\n# testando nossa função:\n\nfor x in x_train_sub[:10]:\n    print(my_tokenizer_pos(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer_bow(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Vamos reaproveitar a classe para seleção de atributos usando SVD.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos criar nosso Pipeline. Em resumo, vamos usar o TFIDF Vectorizer e o nosso POS Tagger em paralelo, e depois juntar os atributos para redimensionar usando o SVD.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport scipy\n\nclf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))\n\n\nmy_pipeline = Pipeline([\n                        ('union', FeatureUnion([('bow', TfidfVectorizer(tokenizer=my_tokenizer_bow)),\\\n                                                ('pos', Pipeline([('pos-vect', CountVectorizer(tokenizer=my_tokenizer_pos)), \\\n                                                         ('pos-tfidf', TfidfTransformer())]))\n                                               ])),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])\n\npar = {'clf__estimator__C' : np.logspace(-4, 4, 20)}\n\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='f1_weighted', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos treinar os algoritmos</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_sub.shape)\nprint(y_train.shape)\n\nhyperpar_selector.fit(X=x_train_sub, y=y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = df_test['sentence'].as_matrix()\ny_test = df_test['relation'].as_matrix()\n\ny_test = label_binarize(y_test, classes=df_train['relation'].unique())\n\nx_test_sub_list = []\n\nfor i, row in df_test.iterrows():\n    pos_t1 = row['sentence'].find(row['term1'])\n    pos_t2 = row['sentence'].find(row['term2'])    \n\n    if pos_t1 < pos_t2:\n        len_t1 = len(row['term1'])    \n        x_test_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['term2'])    \n        x_test_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n    \n\nx_test_sub = np.array(x_test_sub_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted = hyperpar_selector.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_predicted, target_names=df_train['relation'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<p><b>Exercício 7:</b> Treine um modelo de extração de relacionamentos em Português, utilizando o corpus extraído do DBPedia e com relacionamentos entre pares de entidades anotadas.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download pt_core_news_sm","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pt_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz#egg=pt_core_news_sm==2.1.0\n\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz (12.8MB)\n\u001b[K    100% |████████████████████████████████| 12.9MB 98.2MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pt-core-news-sm\n  Running setup.py install for pt-core-news-sm ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed pt-core-news-sm-2.1.0\n\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('pt_core_news_sm')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pt_core_news_sm\nnlp = pt_core_news_sm.load()\nimport nltk\nfrom nltk.stem import RSLPStemmer","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(action='ignore')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = False\nlista = []\nwith open('../input/dbpedia-with-entity-relations-in-portuguese/DBpediaRelations-PT-0.2.txt') as file:\n    for line in file:\n        if(line.strip() == '*********'):\n            break\n        if(line.strip() == '****************************'):\n            if (not start):\n                dic = {}\n            else:\n                #print(dic)\n                lista.append(dic)\n            start = ~start\n        else:\n            if (start):\n                if (line.strip() != ''):\n                    #print(line)\n                    aux = line.split(':')\n                    if(len(aux) < 2):\n                        print(aux)\n                    dic[aux[0].strip()] = aux[1].strip()  ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nrandom.seed(42)\nporcentagem = 0.7\nmask = np.ones(int(len(lista)*porcentagem))\nmask = np.concatenate((mask, np.zeros(len(lista) - len(mask)))) == 1\nrandom.shuffle(mask)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lista = pd.DataFrame(lista)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino = lista[mask]\nteste = lista[~mask]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"teste[:10]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"            ENTITY1       ENTITY2    ...        TYPE1     TYPE2\n2    América Latina        Brasil    ...     LOCATION  LOCATION\n5   Albert Einstein           Ulm    ...       PERSON  LOCATION\n14      Afeganistão      Kandahar    ...     LOCATION  LOCATION\n15      Afeganistão  Hamid Karzai    ...     LOCATION    PERSON\n17           Angola        Luanda    ...     LOCATION  LOCATION\n18           Angola        Luanda    ...     LOCATION  LOCATION\n19           Angola        Luanda    ...     LOCATION  LOCATION\n23         Amazonas        Manaus    ...     LOCATION  LOCATION\n27         Amazonas         Maués    ...     LOCATION  LOCATION\n29         Amazonas        Manaus    ...     LOCATION  LOCATION\n\n[10 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ENTITY1</th>\n      <th>ENTITY2</th>\n      <th>MANUALLY CHECKED</th>\n      <th>REL TYPE</th>\n      <th>SENTENCE</th>\n      <th>TYPE1</th>\n      <th>TYPE2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>América Latina</td>\n      <td>Brasil</td>\n      <td>TRUE</td>\n      <td>locatedInArea</td>\n      <td>A América Latina destaca-se ainda por sua prod...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Albert Einstein</td>\n      <td>Ulm</td>\n      <td>TRUE</td>\n      <td>origin</td>\n      <td>É em Ulm que nasce Albert Einstein, em 14 de m...</td>\n      <td>PERSON</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Afeganistão</td>\n      <td>Kandahar</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>A capital do Afeganistão foi transferida em 17...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Afeganistão</td>\n      <td>Hamid Karzai</td>\n      <td>TRUE</td>\n      <td>keyPerson</td>\n      <td>Em dezembro de 2001 o Conselho de Segurança da...</td>\n      <td>LOCATION</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Angola</td>\n      <td>Luanda</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>O nome Angola é uma derivação portuguesa do te...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Angola</td>\n      <td>Luanda</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>No dia 11 de novembro de 1975 foi proclamada a...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Angola</td>\n      <td>Luanda</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>Em Angola, e mais especialmente em Luanda, a e...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Amazonas</td>\n      <td>Manaus</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>Ganhou a condição de Província do Amazonas pel...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Amazonas</td>\n      <td>Maués</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>Chegaram ao Amazonas os primeiros imigrantes, ...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Amazonas</td>\n      <td>Manaus</td>\n      <td>TRUE</td>\n      <td>other</td>\n      <td>O Amazonas possui várias instituições educacio...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino[:10]","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                   ENTITY1      ENTITY2    ...        TYPE1     TYPE2\n0           América Latina    Argentina    ...     LOCATION  LOCATION\n1           América Latina       Brasil    ...     LOCATION  LOCATION\n3          Albert Einstein          Ulm    ...       PERSON  LOCATION\n4          Albert Einstein  Württemberg    ...       PERSON  LOCATION\n6                  Adriano      Trajano    ...       PERSON    PERSON\n7                  Adriano       Itália    ...       PERSON  LOCATION\n8                  Adriano      Trajano    ...       PERSON    PERSON\n9                  Adriano      Trajano    ...       PERSON    PERSON\n10  Dom Afonso de Portugal       Lisboa    ...       PERSON  LOCATION\n11  Dom Afonso de Portugal     Santarém    ...       PERSON  LOCATION\n\n[10 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ENTITY1</th>\n      <th>ENTITY2</th>\n      <th>MANUALLY CHECKED</th>\n      <th>REL TYPE</th>\n      <th>SENTENCE</th>\n      <th>TYPE1</th>\n      <th>TYPE2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>América Latina</td>\n      <td>Argentina</td>\n      <td>TRUE</td>\n      <td>locatedInArea</td>\n      <td>A América Latina localiza-se totalmente no hem...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>América Latina</td>\n      <td>Brasil</td>\n      <td>TRUE</td>\n      <td>locatedInArea</td>\n      <td>A América Latina não apresenta, ao contrário d...</td>\n      <td>LOCATION</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Albert Einstein</td>\n      <td>Ulm</td>\n      <td>TRUE</td>\n      <td>origin</td>\n      <td>Albert Einstein nasceu na região alemã de Würt...</td>\n      <td>PERSON</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Albert Einstein</td>\n      <td>Württemberg</td>\n      <td>TRUE</td>\n      <td>origin</td>\n      <td>Albert Einstein nasceu na região alemã de Würt...</td>\n      <td>PERSON</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Adriano</td>\n      <td>Trajano</td>\n      <td>TRUE</td>\n      <td>successor</td>\n      <td>Nascido em Itálica na atual Espanha, ou em Rom...</td>\n      <td>PERSON</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Adriano</td>\n      <td>Itália</td>\n      <td>TRUE</td>\n      <td>origin</td>\n      <td>Nascido em Itálica na atual Espanha, ou em Rom...</td>\n      <td>PERSON</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Adriano</td>\n      <td>Trajano</td>\n      <td>TRUE</td>\n      <td>successor</td>\n      <td>Talvez por entender que o império esgotara sua...</td>\n      <td>PERSON</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Adriano</td>\n      <td>Trajano</td>\n      <td>TRUE</td>\n      <td>successor</td>\n      <td>Adriano também retificou os limites de uma out...</td>\n      <td>PERSON</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Dom Afonso de Portugal</td>\n      <td>Lisboa</td>\n      <td>TRUE</td>\n      <td>origin</td>\n      <td>Dom Afonso de Portugal (Lisboa, 18 de Maio de ...</td>\n      <td>PERSON</td>\n      <td>LOCATION</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Dom Afonso de Portugal</td>\n      <td>Santarém</td>\n      <td>TRUE</td>\n      <td>deathOrBurialPlace</td>\n      <td>Dom Afonso de Portugal (Lisboa, 18 de Maio de ...</td>\n      <td>PERSON</td>\n      <td>LOCATION</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(lista)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"98023"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino['REL TYPE'].unique()\n\nx_train = treino['SENTENCE'].as_matrix()\ny_train = treino['REL TYPE'].as_matrix()\n\nfrom sklearn.preprocessing import label_binarize\n\ny_train = label_binarize(y_train, classes=treino['REL TYPE'].unique())\n\nprint(x_train[:10])\nprint(y_train[:10])","execution_count":11,"outputs":[{"output_type":"stream","text":"['A América Latina localiza-se totalmente no hemisfério ocidental, sendo atravessada pelo Trópico de Câncer, que corta a parte central do México; pelo Equador, que passa pelo Brasil, Colômbia, Equador e toca o norte do Peru; e pelo Trópico de Capricórnio, que atravessa o Brasil, o Paraguai, a Argentina e o Chile.'\n 'A América Latina não apresenta, ao contrário da América do Norte, grandes extensões lacustres, mas ainda assim possui inúmeras lagoas costeiras, sobretudo na vertente atlântica, como a lagoa dos Patos, no Brasil; lagoas de inundação nas planícies Amazônica e do Orinoco; e lagos de altitude, como o Titicaca, entre o Peru e a Bolívia.'\n 'Albert Einstein nasceu na região alemã de Württemberg, na cidade de Ulm, numa família judaica.'\n 'Albert Einstein nasceu na região alemã de Württemberg, na cidade de Ulm, numa família judaica.'\n 'Nascido em Itálica na atual Espanha, ou em Roma, na Itália, Adriano era descendente de colonos romanos domiciliados no Sul da Hispânia e primo de Trajano, tendo sido nomeado por este para uma série de dignidades públicas que o fizeram aparecer como herdeiro presuntivo deste imperador.'\n 'Nascido em Itálica na atual Espanha, ou em Roma, na Itália, Adriano era descendente de colonos romanos domiciliados no Sul da Hispânia e primo de Trajano, tendo sido nomeado por este para uma série de dignidades públicas que o fizeram aparecer como herdeiro presuntivo deste imperador.'\n 'Talvez por entender que o império esgotara sua capacidade de expansão, Adriano abandonou a política de conquistas de Trajano, adotando outra nitidamente defensiva, optando pela via diplomática para resolver questões relativas ao relacionamento com povos vizinhos.'\n 'Adriano também retificou os limites de uma outra conquista de Trajano, esta já antiga, a Dácia (atual Roménia), cedendo aos sármatas a planície do Baixo Danúbio e concentrando a ocupação romana na região da Transilvânia, protegida pela barreira natural dos Cárpatos.'\n 'Dom Afonso de Portugal (Lisboa, 18 de Maio de 1475 – Santarém, 13 de Julho de 1491) era filho único.'\n 'Dom Afonso de Portugal (Lisboa, 18 de Maio de 1475 – Santarém, 13 de Julho de 1491) era filho único.']\n[[1 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_sub_list = []\n\nfor i, row in treino.iterrows():\n    pos_t1 = row['SENTENCE'].find(row['ENTITY1'])\n    pos_t2 = row['SENTENCE'].find(row['ENTITY2'])    \n    \n    if pos_t1 < pos_t2:\n        len_t1 = len(row['ENTITY1'])\n        x_train_sub_list.append(row['SENTENCE'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['ENTITY2'])\n        x_train_sub_list.append(row['SENTENCE'][pos_t2+len_t2:pos_t1])\n        \nx_train_sub = np.array(x_train_sub_list)\n\nprint(x_train_sub[:10])","execution_count":19,"outputs":[{"output_type":"stream","text":"[' localiza-se totalmente no hemisfério ocidental, sendo atravessada pelo Trópico de Câncer, que corta a parte central do México; pelo Equador, que passa pelo Brasil, Colômbia, Equador e toca o norte do Peru; e pelo Trópico de Capricórnio, que atravessa o Brasil, o Paraguai, a '\n ' não apresenta, ao contrário da América do Norte, grandes extensões lacustres, mas ainda assim possui inúmeras lagoas costeiras, sobretudo na vertente atlântica, como a lagoa dos Patos, no '\n ' nasceu na região alemã de Württemberg, na cidade de '\n ' nasceu na região alemã de '\n ' era descendente de colonos romanos domiciliados no Sul da Hispânia e primo de '\n ', ' ' abandonou a política de conquistas de '\n ' também retificou os limites de uma outra conquista de ' ' ('\n ' (Lisboa, 18 de Maio de 1475 – ']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\ndef my_tokenizer_pos(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    return [pos[1] for pos in pos_tags]\n\n# testando nossa função:\n\nfor x in x_train_sub[:10]:\n    print(my_tokenizer_pos(x))","execution_count":20,"outputs":[{"output_type":"stream","text":"['JJ', 'NN', 'DT', 'NN', 'NN', ',', 'VBP', 'JJ', 'NN', 'NNP', 'FW', 'NNP', ',', 'NN', 'VBD', 'DT', 'JJ', 'JJ', 'VBP', 'NNP', ':', 'NN', 'NNP', ',', 'NN', 'NN', 'NN', 'NNP', ',', 'NNP', ',', 'NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'VBP', 'NNP', ':', 'CC', 'VB', 'NNP', 'FW', 'NNP', ',', 'NN', 'NN', 'NN', 'NNP', ',', 'NN', 'NNP', ',', 'DT']\n['JJ', 'NN', ',', 'VBP', 'NN', 'NN', 'NNP', 'VBP', 'NNP', ',', 'VBZ', 'JJ', 'NNS', ',', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'NNS', ',', 'NN', 'TO', 'NN', 'NN', ',', 'VB', 'DT', 'JJ', 'NN', 'NNP', ',', 'DT']\n['RB', 'JJ', 'NN', 'NN', 'IN', 'NNP', ',', 'RB', 'NN', 'IN']\n['RB', 'JJ', 'NN', 'NN', 'IN']\n['NN', 'NN', 'IN', 'FW', 'NNS', 'VBP', 'DT', 'NNP', 'VBZ', 'NNP', 'NN', 'NN', 'IN']\n[',']\n['VB', 'DT', 'NN', 'IN', 'FW', 'FW']\n['NN', 'NN', 'JJ', 'VBZ', 'FW', 'FW', 'JJ', 'NN', 'IN']\n['(']\n['(', 'NNP', ',', 'CD', 'FW', 'NNP', 'IN', 'CD', 'NN']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_list = stopwords.words('portuguese')\n\nstemmer = nltk.stem.RSLPStemmer()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_tokenizer_bow(doc):\n    words = word_tokenize(doc)\n    \n    non_stopwords = [w for w in words if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n                \n        lemmas.append(stemmer.stem(w))\n\n    return lemmas","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport scipy\n\nclf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))\n\n\nmy_pipeline = Pipeline([\n                        ('union', FeatureUnion([('bow', TfidfVectorizer(tokenizer=my_tokenizer_bow)),\\\n                                                ('pos', Pipeline([('pos-vect', CountVectorizer(tokenizer=my_tokenizer_pos)), \\\n                                                         ('pos-tfidf', TfidfTransformer())]))\n                                               ])),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])\n\npar = {'clf__estimator__C' : np.logspace(-4, 4, 20)}\n\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='f1_weighted', n_jobs=1, n_iter=20)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_sub.shape)\nprint(y_train.shape)\n\nhyperpar_selector.fit(X=x_train_sub[:500], y=y_train[:500])","execution_count":25,"outputs":[{"output_type":"stream","text":"(68616,)\n(68616, 10)\n","name":"stdout"},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n          estimator=Pipeline(memory=None,\n     steps=[('union', FeatureUnion(n_jobs=None,\n       transformer_list=[('bow', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_r...tate=0, solver='lbfgs',\n          tol=0.0001, verbose=0, warm_start=False),\n          n_jobs=None))]),\n          fit_params=None, iid='warn', n_iter=20, n_jobs=1,\n          param_distributions={'clf__estimator__C': array([1.00000e-04, 2.63665e-04, 6.95193e-04, 1.83298e-03, 4.83293e-03,\n       1.27427e-02, 3.35982e-02, 8.85867e-02, 2.33572e-01, 6.15848e-01,\n       1.62378e+00, 4.28133e+00, 1.12884e+01, 2.97635e+01, 7.84760e+01,\n       2.06914e+02, 5.45559e+02, 1.43845e+03, 3.79269e+03, 1.00000e+04])},\n          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n          return_train_score='warn', scoring='f1_weighted', verbose=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = teste['SENTENCE'].as_matrix()\ny_test = teste['REL TYPE'].as_matrix()\n\ny_test = label_binarize(y_test, classes=treino['REL TYPE'].unique())\n\nx_test_sub_list = []\n\nfor i, row in teste.iterrows():\n    pos_t1 = row['SENTENCE'].find(row['ENTITY1'])\n    pos_t2 = row['SENTENCE'].find(row['ENTITY2'])    \n\n    if pos_t1 < pos_t2:\n        len_t1 = len(row['ENTITY1'])    \n        x_test_sub_list.append(row['SENTENCE'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['ENTITY2'])    \n        x_test_sub_list.append(row['SENTENCE'][pos_t2+len_t2:pos_t1])\n    \n\nx_test_sub = np.array(x_test_sub_list)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted = hyperpar_selector.predict(x_test)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_predicted, target_names=treino['REL TYPE'].unique()))","execution_count":30,"outputs":[{"output_type":"stream","text":"                    precision    recall  f1-score   support\n\n     locatedInArea       0.69      0.31      0.43     14047\n            origin       0.54      0.00      0.01      7881\n         successor       0.00      0.00      0.00       160\ndeathOrBurialPlace       0.54      0.40      0.46      2157\n             other       0.13      0.13      0.13      3165\n         keyPerson       0.00      0.00      0.00       116\n            partOf       0.17      0.00      0.01      1663\n      influencedBy       0.00      0.00      0.00        51\n           partner       0.00      0.00      0.00        65\n            parent       0.00      0.00      0.00       102\n\n         micro avg       0.50      0.19      0.28     29407\n         macro avg       0.21      0.09      0.10     29407\n      weighted avg       0.54      0.19      0.26     29407\n       samples avg       0.19      0.19      0.19     29407\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}