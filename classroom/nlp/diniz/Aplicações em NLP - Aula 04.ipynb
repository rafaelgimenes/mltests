{"cells":[{"metadata":{"_uuid":"a43e5be009b780511602d16cd27092010defd8ed"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"1431c747ca4eb2b7369c12164d3f81f36a959d96"},"cell_type":"markdown","source":"<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n<h2 align=\"center\"> Aula 04 - Classificação de Texto</h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>"},{"metadata":{"_uuid":"bd84a5d56371f457ae8a987869b3ef6fa67135a3"},"cell_type":"markdown","source":"<h2>Problema de Classificação</h2>"},{"metadata":{"_uuid":"9dcda86d9f553ffa0cf47339d7b5b60f0dee5744"},"cell_type":"markdown","source":"<p>Neste tutorial vamos trabalhar com um exemplo prático de problema de classificação de texto. O objetivo é identificar uma sentença como escrita \"formal\" ou \"informal\".</p>"},{"metadata":{"_uuid":"fcfcd8a08ec7ab0d6fbe0caa72b8f1b6f853f4a6"},"cell_type":"markdown","source":"<b>1. Obtendo o corpus</b>"},{"metadata":{"_uuid":"1b6281b1f25821ac391ae188d9dfea135c2bfa80"},"cell_type":"markdown","source":"<p>Para simplificar o problema, vamos continuar utilizando o corpus Gutenberg como textos formais e vamos usar mensagens de chat do corpus <b>nps_chat</b> como textos informais.</p>\n<p>Antes de tudo, vamos baixar o corpus nps_chat:</p>"},{"metadata":{"trusted":true,"_uuid":"15bc1ed84dbd5bf8b5f9abefaa387d6e0334351a"},"cell_type":"code","source":"import nltk\n\nnltk.download('nps_chat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"405d211dd4759eb321016f901969ef5c8a3ff890"},"cell_type":"code","source":"from nltk.corpus import nps_chat\n\nprint(nps_chat.fileids())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a052a69d2827dee142b5f4796e127d76545dc5a"},"cell_type":"markdown","source":"<p>Agora vamos ler os dois corpus e armazenar as sentenças em uma mesma ndarray. Perceba que também teremos uma ndarray para indicar se o texto é formal ou não. Começamos armazenando o corpus em lists. Vamos usar apenas 500 elementos de cada, para fins didáticos.</p>"},{"metadata":{"trusted":true,"_uuid":"5b0ba72389d01345262696be36c041804731d68e"},"cell_type":"code","source":"import nltk\n\nx_data_nps = []\n\nfor fileid in nltk.corpus.nps_chat.fileids():#corre os filtros\n    x_data_nps.extend([post.text for post in nps_chat.xml_posts(fileid)])\n\ny_data_nps = [0] * len(x_data_nps)#CRIAR MATRIZ DE 0 do mesmo tamanho q o dataset\n\nx_data_gut = []\n#uni as palavras pra montar a sentença depois tokeniza\nfor fileid in nltk.corpus.gutenberg.fileids():\n    x_data_gut.extend([' '.join(sent) for sent in nltk.corpus.gutenberg.sents(fileid)])#.sents-segmentsdo\n    \ny_data_gut = [1] * len(x_data_gut)\n\nx_data_full = x_data_nps[:500] + x_data_gut[:500]\nprint(len(x_data_full))\ny_data_full = y_data_nps[:500] + y_data_gut[:500]\nprint(len(y_data_full))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#10 primeiros\nprint(x_data_full[:10])\nprint(y_data_full[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#10 últimos\nprint(x_data_full[-10:])\nprint(y_data_full[-10:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3af90a8a78afe244b83bb4b471e31e31d67e4875"},"cell_type":"markdown","source":"<p>Em seguida, transformamos essas listas em ndarrays, para usarmos nas etapas de pré-processamento que já conhecemos.</p>"},{"metadata":{"trusted":true,"_uuid":"bcb63d9bfaf909414bdd3d3618630112d9e2168f"},"cell_type":"code","source":"import numpy as np\n\nx_data = np.array(x_data_full, dtype=object)\n#x_data = np.array(x_data_full)\nprint(x_data.shape)\ny_data = np.array(y_data_full)\nprint(y_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ab3dd8f3bb0bd5677cf2698b2d649fbab8d9ed"},"cell_type":"markdown","source":"<b>2. Dividindo em datasets de treino e teste</b>"},{"metadata":{"_uuid":"e2095721ee2c652138f83bfc4c1aab2909917f76"},"cell_type":"markdown","source":"<p>Para que a pesquisa seja confiável, precisamos avaliar os resultados em um dataset de teste. Por isso, vamos dividir os dados aleatoriamente, deixando 80% para treino e o demais para testar os resultados em breve.</p>"},{"metadata":{"trusted":true,"_uuid":"08d308234980d9f76615330a01c31eba2d6b9c7a"},"cell_type":"code","source":"train_indexes = np.random.rand(len(x_data)) < 0.80\n\nprint(len(train_indexes))\nprint(train_indexes[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f545362ec7b480649e47760973a0252a3419b8b"},"cell_type":"code","source":"x_data_train = x_data[train_indexes]\ny_data_train = y_data[train_indexes]\n\nprint(len(x_data_train))\nprint(len(y_data_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a43b65190ba1347ad87ebfd94c532962052d3b7b"},"cell_type":"code","source":"x_data_test = x_data[~train_indexes]\ny_data_test = y_data[~train_indexes]\n\nprint(len(x_data_test))\nprint(len(y_data_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e1cbc5c27027dead20df3f5e304e156de82020"},"cell_type":"markdown","source":"<b>3. Treinando o classificador</b>"},{"metadata":{"_uuid":"1ab54e667b94d390c0953652fa8d4260bf57edbf"},"cell_type":"markdown","source":"<p>Para tokenização, vamos usar a mesma função do tutorial anterior:</p>"},{"metadata":{"trusted":true,"_uuid":"fecc5788bdea938acbcf20af3185d12a60a5f29b"},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nstopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):#doc=uma sentença\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:#converte os postab para o que tem no wordnet\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49296c720901fa5fad6206c0da34271b2d3c9b1d"},"cell_type":"markdown","source":"<p>Mas agora vamos criar um <b>pipeline</b> contendo o vetorizador TF-IDF, o SVD para redução de atributos e um algoritmo de classificação. Mas antes, vamos encapsular nosso algoritmo para escolher o número de dimensões para o SVD em uma classe que pode ser utilizada com o pipeline:</p>"},{"metadata":{"trusted":true,"_uuid":"7f5a4228413c0ff9bf3fff1761a40abacdc68de7"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))#2000 componentes\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:#ordenado inverso(do maior para o menor)r)\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:#50% da representação da variância no SVD\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1687b8e4695924cb6394b4d01a065b6976460169"},"cell_type":"markdown","source":"<p>Finalmente podemos criar nosso pipeline:</p>"},{"metadata":{"trusted":true,"_uuid":"9c291497b515efaddd76ad8b11080b50cb7760ef"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import neighbors\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\nmy_pipeline = Pipeline([('tfidf', TfidfVectorizer(tokenizer=my_tokenizer)),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f199bd69edebf53f3fa28f6bee8676112666b17d"},"cell_type":"markdown","source":"<p>Estamos quase lá... Agora vamos criar um objeto <b>RandomizedSearchCV</b> que fará a seleção de hiper-parâmetros do nosso classificador (aka. parâmetros que não são aprendidos durante o treinamento). Essa etapa é importante para obtermos a melhor configuração do algoritmo de classificação. Para economizar tempo de treinamento, vamos usar um algoritmo simples o <i>K nearest neighbors (KNN)</i>."},{"metadata":{"trusted":true,"_uuid":"737d398afd745d784a48153b3be977dd72398791"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nimport scipy\n\npar = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=2, n_iter=20)#cv = 3 folders do cross validation\n#n_jobs = n° de threads, n_iter = 20 parâmetros","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"021f54a2e324792f07117ee06d7575ecfd7bc975"},"cell_type":"markdown","source":"<p>E agora vamos treinar nosso algoritmo, usando o pipeline com seleção de atributos:</p>"},{"metadata":{"trusted":true,"_uuid":"51f5f97e12a6e51fb021d214171580e23684a994"},"cell_type":"code","source":"print(x_data_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"a003c3c4c94751651036a8f150a65be1e091bb06"},"cell_type":"code","source":"hyperpar_selector.fit(X=x_data_train, y=y_data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff27420b2baa60b2c4807cbeecce5efa8bca070b"},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nfor param_name in sorted(par.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01509c3f9e2eaa500b7f8e161be1d8e785251fc9"},"cell_type":"markdown","source":"<b>4. Testando o classificador</b>"},{"metadata":{"_uuid":"d5296fbf715ca56fc752089af7b2af00191a09d5"},"cell_type":"markdown","source":"<p>Agora vamos usar o classificador com o nosso dataset de testes, e observar os resultados:</p>"},{"metadata":{"trusted":true,"_uuid":"cd87421c099a977a993cca07145054ef5ba113cc"},"cell_type":"code","source":"from sklearn.metrics import *\n\ny_pred = hyperpar_selector.predict(x_data_test)\n\nprint(accuracy_score(y_data_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8644502f3117d2ad16e9e278570f16f844e06aa"},"cell_type":"markdown","source":"<b>5. Serializando o modelo</b><br>"},{"metadata":{"trusted":true,"_uuid":"04b8d4e75d69d456a92d496636335197a95f5e7f"},"cell_type":"code","source":"import pickle\n\nstring_obj = pickle.dumps(hyperpar_selector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f777e622dee0cda85453ba6ff90d762df4c37a4"},"cell_type":"code","source":"model_file = open('model.pkl', 'wb')\n\nmodel_file.write(string_obj)\n\nmodel_file.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae7a3b02e732f9884697d90a8841b643bf7d5e33"},"cell_type":"markdown","source":"<b>6. Abrindo e usando um modelo salvo </b><br>"},{"metadata":{"trusted":true,"_uuid":"7cda7cfbd4b3543c04197cc17636a5bd5f439798"},"cell_type":"code","source":"\nmodel_file = open('model.pkl', 'rb')\nmodel_content = model_file.read()\n\nobj_classifier = pickle.loads(model_content)\n\nmodel_file.close()\n\nres = obj_classifier.predict([\"what's up bro?\"])\n\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e205c4248f847362e35b0db275a10456bad4bd"},"cell_type":"code","source":"res = obj_classifier.predict(x_data_test)\nprint(accuracy_score(y_data_test, res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4179e49865206470a6974703bb81c58e7a57161f"},"cell_type":"code","source":"res = obj_classifier.predict(x_data_test)\n\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18f7bc380f9bd026fef63efd76e6e58b601aed63"},"cell_type":"code","source":"formal = [x_data_test[i] for i in range(len(res)) if res[i] == 1]\n\nfor txt in formal:\n    print(\"%s\\n\" % txt)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"640d327d1e766e1e919497432ef1d17bae926c37"},"cell_type":"code","source":"informal = [x_data_test[i] for i in range(len(res)) if res[i] == 0]\n\nfor txt in informal:\n    print(\"%s\\n\" % txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf1536c256269a8933315b40a448b91116e9fcf6"},"cell_type":"code","source":"res2 = obj_classifier.predict([\"Emma spared no exertions to maintain this happier flow of ideas , and hoped , by the help of backgammon , to get her father tolerably through the evening , and be attacked by no regrets but her own\"])\n\nprint(res2)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"84bec3534ea03eae2f3cd37617a48520c60aba0a"},"cell_type":"markdown","source":"<p><b>Exercício 4:</b>  Treine um modelo para classificar sentenças de acordo com o sentimento (positivo ou negativo), utilizando o corpus sentence_polarity do nltk.\n    Dica: Para obter sentenças positivas use:\n</p>"},{"metadata":{"trusted":true,"_uuid":"8ffa5cf345b4c00e1d98b67574989ef8ad4381ed"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nnltk.download('sentence_polarity')\nfrom nltk.corpus import sentence_polarity\n\nsentence_polarity.sents(categories=['pos'])","execution_count":15,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package sentence_polarity to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package sentence_polarity is already up-to-date!\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"[['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', \"century's\", 'new', '\"', 'conan', '\"', 'and', 'that', \"he's\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of', 'the', 'rings', '\"', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co-writer/director', 'peter', \"jackson's\", 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', \"tolkien's\", 'middle-earth', '.'], ...]"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"4930689c3fdc9c53a1684f0f4653e423f237f5b8"},"cell_type":"code","source":"print(len(sentence_polarity.sents(categories=['neg'])))\nprint(len(sentence_polarity.sents(categories=['pos'])))","execution_count":16,"outputs":[{"output_type":"stream","text":"5331\n5331\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\n#x_data_neg = sentence_polarity.sents(categories=['neg'])\nx_data_neg = []\nx_data_neg.extend([' '.join(sent) for sent in sentence_polarity.sents(categories=['neg'])])\n\ny_data_neg = [0] * len(sentence_polarity.sents(categories=['neg']))\n\n#x_data_pos = sentence_polarity.sents(categories=['pos'])\nx_data_pos = []\nx_data_pos.extend([' '.join(sent) for sent in sentence_polarity.sents(categories=['pos'])])\n\ny_data_pos = [1] * len(sentence_polarity.sents(categories=['pos']))\n\nx_data_full = x_data_neg[:1000] + x_data_pos[:1000]\nprint(len(x_data_full))\ny_data_full = y_data_neg[:1000] + y_data_pos[:1000]\nprint(len(y_data_full))","execution_count":17,"outputs":[{"output_type":"stream","text":"2000\n2000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nx_data = np.array(x_data_full, dtype=object)\n#x_data = np.array(x_data_full)\nprint(x_data.shape)\ny_data = np.array(y_data_full)\nprint(y_data.shape)","execution_count":18,"outputs":[{"output_type":"stream","text":"(2000,)\n(2000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_indexes = np.random.rand(len(x_data)) < 0.80\n\nprint(len(train_indexes))\nprint(train_indexes[:10])","execution_count":19,"outputs":[{"output_type":"stream","text":"2000\n[ True False  True  True  True  True False False  True  True]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data_train = x_data[train_indexes]\ny_data_train = y_data[train_indexes]\n\nprint(len(x_data_train))\nprint(len(y_data_train))","execution_count":20,"outputs":[{"output_type":"stream","text":"1604\n1604\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data_test = x_data[~train_indexes]\ny_data_test = y_data[~train_indexes]\n\nprint(len(x_data_test))\nprint(len(y_data_test))","execution_count":21,"outputs":[{"output_type":"stream","text":"396\n396\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nstopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):#doc=uma sentença\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:#converte os postab para o que tem no wordnet\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas\n    \n    ","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))#2000 componentes\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:#ordenado inverso(do maior para o menor)r)\n                cummulative_variance += var\n                if cummulative_variance >= 0.8:#50% da representação da variância no SVD\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import neighbors\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\nmy_pipeline = Pipeline([('tfidf', TfidfVectorizer(tokenizer=my_tokenizer)),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nimport scipy\n\npar = {'clf__n_neighbors': range(1, 100), 'clf__weights': ['uniform', 'distance']}\n\n\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=40)#cv = 3 folders do cross validation\n","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=x_data_train, y=y_data_train)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n          estimator=Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...i',\n           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n           weights='uniform'))]),\n          fit_params=None, iid='warn', n_iter=40, n_jobs=1,\n          param_distributions={'clf__n_neighbors': range(1, 100), 'clf__weights': ['uniform', 'distance']},\n          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n          return_train_score='warn', scoring='accuracy', verbose=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nfor param_name in sorted(par.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":27,"outputs":[{"output_type":"stream","text":"Best score: 0.658\nBest parameters set:\n\tclf__n_neighbors: 58\n\tclf__weights: 'distance'\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import *\n\ny_pred = hyperpar_selector.predict(x_data_test)\n\nprint(accuracy_score(y_data_test, y_pred))","execution_count":28,"outputs":[{"output_type":"stream","text":"0.648989898989899\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#aumentar os dados\n#aumentar a variância\n#aumentar os parâmetros\n#aumentar o range dos parâmetros","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}