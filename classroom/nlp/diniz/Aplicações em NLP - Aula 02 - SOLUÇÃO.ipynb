{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "428a2b6453f387b27ac6dd44f4863ddfa3ef9570"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2440ac30303a1254a37d1d01ee785057bb97b6ef"
   },
   "source": [
    "<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n",
    "<h2 align=\"center\"> Aula 02 - Técnicas de Pré-Processamento de Texto</h2>\n",
    "<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66a45a0ef142fe11163dc0f60e32ac3bd27b24f2"
   },
   "source": [
    "<h2> Técnicas para Pré-Processamento </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e5e103ce1fbe2af13cdb92fe3ee13a681745a72"
   },
   "source": [
    "<p>Vamos avaliar as técnicas mais comuns para prepararmos o texto para usar com algoritmos de aprendizado de máquina logo mais.</p>\n",
    "<p>Como estudo de caso, vamos usar o texto de <i>Hamlet</i>, encontrado no corpus <i>Gutenberg</i> do pacote <b>NLTK</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df15ee3ae4d9d6b1b693c1e67e608490ee1b31ab"
   },
   "source": [
    "<b>1. Baixando o corpus Gutenberg</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f54a277d7198b10007bf8831da504ff578fb14df"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "224b664ca10d58ee7ac4459ae9e004cd0f83f66e"
   },
   "source": [
    "<b>2. Exibindo o texto \"Hamlet\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6c0aa6cfc8945728fb0c49fec3e81e1056dbfac"
   },
   "outputs": [],
   "source": [
    "hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n",
    "print(hamlet_raw[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4cbee0ddf6925f591e8609587ea33c63babc1bb"
   },
   "source": [
    "<b>3. Segmentação de sentenças e tokenização de palavras</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55f5333efcce3d3950bca71579068896d3bf258c"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(hamlet_raw)\n",
    "\n",
    "print(sentences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf27b536c1c965754d532c000cd2397c1695f4ef"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(sentences[0])\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a3f0fb3489888df524d2a8c088dd769c47cdc9a"
   },
   "source": [
    "<b>4. Removendo stopwords e pontuação</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e9e941dcabcf24cfc85410337c92810f9830cc4"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04b5fb309eb2c23a4f80198e760a247cbfa58907"
   },
   "outputs": [],
   "source": [
    "non_stopwords = [w for w in words if not w.lower() in stopwords_list]\n",
    "print(non_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fe37e5ecda0d2978f9e7af55b1aaa724a0cd356"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b4382b81da60a4757f8b2a5f215d2d610a8d792"
   },
   "outputs": [],
   "source": [
    "non_punctuation = [w for w in non_stopwords if not w in punctuation]\n",
    "\n",
    "print(non_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "079a356c9059f5045d505c61408de4104600c948"
   },
   "source": [
    "<b>5. Part of Speech (POS) Tags </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d6555b6ecb668b159b154ff719708a3484ae32a"
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fefd1fbe51165eff387cb3b06654f15dcf97fcd9"
   },
   "source": [
    "As tags indicam a classificação sintática de cada palavra no texto. Ver <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\" target=\"blank\">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a> para uma lista completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abec7efb2ac6464efa07368ed56e417a4364f77a"
   },
   "source": [
    "<b>6. Stemming e Lemmatization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ad31d9a403031f7ddf07f6ec9ebba9ae99e1fa3"
   },
   "source": [
    "Stemming permite obter a \"raiz\" da palavra, removendo sufixos, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f4884b863698ce641f8517899bf701141578212"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "sample_sentence = \"He has already gone\"\n",
    "sample_words = word_tokenize(sample_sentence)\n",
    "\n",
    "stems = [stemmer.stem(w) for w in sample_words]\n",
    "\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ed008c5fdc8c51d1840c17d97c9af68ac570810"
   },
   "source": [
    "Já lemmatization vai além de somente remover sufixos, obtendo a raiz linguística da palavra. Vamos usar as tags POS obtidas anteriormente para otimizar o lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6223ab4ff6dda4c5972d0e06feb4606d8de79bf1"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2dcec42a137e8ef1b039fecfade061341077f65f"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pos_tags = nltk.pos_tag(sample_words)\n",
    "\n",
    "lemmas = []\n",
    "for w in pos_tags:\n",
    "    if w[1].startswith('J'):\n",
    "        pos_tag = wordnet.ADJ\n",
    "    elif w[1].startswith('V'):\n",
    "        pos_tag = wordnet.VERB\n",
    "    elif w[1].startswith('N'):\n",
    "        pos_tag = wordnet.NOUN\n",
    "    elif w[1].startswith('R'):\n",
    "        pos_tag = wordnet.ADV\n",
    "    else:\n",
    "        pos_tag = wordnet.NOUN\n",
    "        \n",
    "    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n",
    "    \n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5486205553b026fdeaf7ee9614008c45ab15ba0"
   },
   "source": [
    "<b>7. N-gramas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4344cc10eae92b57834efddfadab48fcd6712c29"
   },
   "source": [
    "Além da técnica de <i>Bag-of-Words</i>, outra opção é utilizar n-gramas (onde \"n\" pode variar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2734803dfec24819f1e90295fd3640957bda68a"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "frase = 'o cachorro correu atrás do gato'\n",
    "\n",
    "\n",
    "ngrams = [\"%s %s %s\" % (nltk.word_tokenize(frase)[i], \\\n",
    "                      nltk.word_tokenize(frase)[i+1], \\\n",
    "                      nltk.word_tokenize(frase)[i+2]) \\\n",
    "          for i in range(len(nltk.word_tokenize(frase))-2)]\n",
    "\n",
    "print(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca24505986a23623a06a6e6daefd3db05dd4a339"
   },
   "outputs": [],
   "source": [
    "non_punctuation = [w for w in words if not w.lower() in punctuation]\n",
    "\n",
    "n_grams_3 = [\"%s %s %s\"%(non_punctuation[i], non_punctuation[i+1], non_punctuation[i+2]) for i in range(0, len(non_punctuation)-2)]\n",
    "\n",
    "print(n_grams_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bac2675048fe38fc92515e951c89a03fd29fa35d"
   },
   "source": [
    "Também podemos usar a classe <b>CountVectorizer</b>, do scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b46180ecfd8499075a0e81ffcea419438690071d"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([sentences[0]])\n",
    "\n",
    "print(arr)\n",
    "\n",
    "n_gram_counts = count_vect.fit_transform(arr)\n",
    "\n",
    "print(n_gram_counts.toarray())\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36977ac8fff2cbc51064eaa6aef0a7f86e3d24d5"
   },
   "source": [
    "Agora, vamos contar os n-grams (no nosso caso, trigramas) de todas as sentenças do texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54801f0534adb4a9986e4115fc04a0fdc9deb837"
   },
   "outputs": [],
   "source": [
    "arr = np.array(sentences)\n",
    "\n",
    "n_gram_counts = count_vect.fit_transform(arr)\n",
    "\n",
    "print(n_gram_counts.toarray()[:20])\n",
    "\n",
    "print([k for k in count_vect.vocabulary_.keys()][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "815883db41702bd7cc0e1e1d2f59c72ba84c1468"
   },
   "source": [
    "<p><b>Exercício 2:</b>Exiba 10 lemmas mais frequentes do corpus Reuters, ignorando pontuações e stopwords.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "426a646f42e5564ec070e90e114743a3f1f8d894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8846853\n",
      "[('vs', 5933),\n",
      " ('year', 6379),\n",
      " ('ct', 8096),\n",
      " ('v', 8176),\n",
      " (\"'s\", 8340),\n",
      " ('lt', 8693),\n",
      " ('pct', 9053),\n",
      " ('dlrs', 11697),\n",
      " ('mln', 18011),\n",
      " ('say', 26080)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "reuters_raw_content = ''\n",
    "for fid in reuters.fileids():\n",
    "    reuters_raw_content += reuters.raw(fid)\n",
    "    \n",
    "# Primeiro coletei todo o conteúdo do corpus\n",
    "print(len(reuters_raw_content))\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(reuters_raw_content)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = {}\n",
    "\n",
    "import string\n",
    "punctuation = string.punctuation\n",
    "punctuation += \"```''''\"\n",
    "\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    non_punctuation = [w for w in pos_tags if not w[0].lower() in punctuation]\n",
    "    non_stopwords = [w for w in non_punctuation if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "    for w in non_stopwords:\n",
    "        if w[1].startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif w[1].startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif w[1].startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif w[1].startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = wordnet.NOUN\n",
    "\n",
    "        l = lemmatizer.lemmatize(w[0], pos)\n",
    "        if not l in lemmas.keys():\n",
    "            lemmas[l] = 0\n",
    "        else:\n",
    "            lemmas[l] += 1\n",
    "            \n",
    "import pprint\n",
    "import operator\n",
    "pprint.pprint(sorted(lemmas.items(), key=operator.itemgetter(1))[-10:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d43c9e205955367b1c92a059f359368bc9b52f55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
