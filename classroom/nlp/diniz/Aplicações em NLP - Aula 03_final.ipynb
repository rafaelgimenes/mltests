{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d59e1714618222ce70d6049a2280bca3e71a7b38"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a89f11e17dec224a567a99fe4d45d2ee9f52c196"
   },
   "source": [
    "<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n",
    "<h2 align=\"center\"> Aula 03 - Técnicas de Pré-Processamento de Texto e Similaridade</h2>\n",
    "<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "402e01a1ec45619f8553d15a85ba73d099e93e19"
   },
   "source": [
    "<h2> Técnicas para Pré-Processamento - Parte 2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e70ddf470536e6f629a2e5bac309e1296a881238"
   },
   "source": [
    "<p>Uma vez que o texto já foi devidamente tratado, removendo stopwords e pontuações, e aplicando stemming ou lemmatization, agora precisamos contar a frequência das palavras (ou n-grams) que utilizaremos em seguida como atributos para as técnicas de aprendizado de máquina.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32726855db9b2f9adec59f35af9062eb7a6fd0b3"
   },
   "source": [
    "<b>1. TF-IDF (Term Frequency - Inverse Document Frequency)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec5fa0cecd2471b52b43a81e26626a7d9d172b94"
   },
   "source": [
    "<p><b>Term Frequency:</b> um termo que aparece muito em um documento, tende a ser um termo importante. Em resumo, divide-se o número de vezes em que um termo apareceu pelo maior número de vezes em que algum outro termo apareceu no documento.</p>\n",
    "\n",
    "tf<sub>wd</sub> = f<sub>wd</sub> / m<sub>wd</sub>\n",
    "\n",
    "onde:<br>\n",
    "f<sub>wd</sub> é o número de vezes em que o termo <i>w</i> aparece no documento <i>d</i>.<br>\n",
    "m<sub>wd</sub> é o maior valor de f<sub>wd</sub> obtido para algum termo do documento <i>d</i><br>\n",
    "\n",
    "<p><b>Inverse Document Frequency:</b> um termo que aparece em poucos documentos pode ser um bom descriminante. Obtem-se dividindo o número de documentos pelo número de documentos em que o termo aparece.</p>\n",
    "\n",
    "idf<sub>w</sub> = log<sub>2</sub>(n / n<sub>w</sub>)\n",
    "\n",
    "onde:<br>\n",
    "n é o número de documentos no corpus\n",
    "n<sub>w</sub> é o número de documentos em que o termo <i>w</i> aparece.\n",
    "\n",
    "Na prática, usa-se:\n",
    "\n",
    "tf-idf = tf<sub>wd</sub> * idf<sub>w</sub>\n",
    "\n",
    "Podemos calcular o TF-IDF de um corpus usando o pacote <b>scikit-learn</b>. Primeiramente, vamos abrir novamente o texto de Hamlet e armazenar as sentenças em uma ndarray do numpy (como se cada sentença fosse um documento do corpus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33063b14e207a18933105ced5c4dfc40d2905fa5"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "sents = sent_tokenize(hamlet_raw)\n",
    "\n",
    "hamlet_np = np.array(sents)\n",
    "\n",
    "print(hamlet_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eaf4eabeb753a08246129fbb94435da7e34b966d"
   },
   "source": [
    "<p>Agora vamos definir uma função para tokenização pelo scikit-learn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b54f728cb8a002a4601d17e940720501eb44a000"
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n",
    "    \n",
    "    lemmas = []\n",
    "    for w in non_punctuation:\n",
    "        if w[1].startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif w[1].startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif w[1].startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif w[1].startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = wordnet.NOUN\n",
    "        \n",
    "        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n",
    "\n",
    "    return lemmas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b61e5963b929b8c62b1caa94b29f497a3e453c2"
   },
   "source": [
    "E essa função será chamada pelo objeto TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef6bcf9f1bb0448308dccf8a3a36e1789bce1a9b"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "sents = sent_tokenize(hamlet_raw)\n",
    "\n",
    "hamlet_np = np.array(sents)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "\n",
    "tfs = tfidf_vectorizer.fit_transform(hamlet_np)\n",
    "\n",
    "print(tfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a407bd2272880203b2ce42a74e6818d3d18c3a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print([k for k in tfidf_vectorizer.vocabulary_.keys()][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8b7f5724b7e641cd5a0656c7ea77dc40d6e7dae"
   },
   "outputs": [],
   "source": [
    "print(tfs[:50,:50])#linha 0 coluna 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b891d5d8811a6149e10d816c5214f26c8672e7b1"
   },
   "source": [
    "<b>2. TF-IDF de N-gramas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "515cfc8aedda7741940d22adbdf6169654a94139"
   },
   "source": [
    "Opcionalmente, podemos obter os atributos tf-idf de n-grams, combinando as classes CountVectorizer e TfidfTransformer. Em nosso exemplo, vamos utilizar apenas trigramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e48fff7b4ce29a920bae36331361214b5eb2e5d"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "n_gram_counts = count_vect.fit_transform(hamlet_np)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfs_ngrams = tfidf_transformer.fit_transform(n_gram_counts)\n",
    "\n",
    "print(tfs_ngrams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1269c71145cd2f3ce22961fc945bf9761646249c"
   },
   "source": [
    "<b>3. Redução de Dimensionalidade</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fab659cec6d0f4b34a6a2dd9b1aacee2a46100b"
   },
   "source": [
    "<p>A transformação do corpus em atributos contendo as frequências TF-IDF em geral resultará numa ndarray bastante esparsa, ou seja, com muitas dimensões. Porém, além de isso tornar o treinamento de algoritmos mais demorado e custoso (computacionalmente falando), muitas dessas dimensões provavelmente são pouco representativas ou mesmo podem causar ruído durante o treinamento. Para resolver esse problema, podemos aplicar uma técnica de redução de dimensionalidade simples chamada <b>Singular Value Decomposition (SVD)</b>. \n",
    "\n",
    "<p>Essa técnica transformará os vetores da matriz original, rotacionando e escalando-os, resultando em novas representações. A redução de dimensionalidade é feita ao manter apenas as <i>k</i> dimensões mais representativas que escolhermos. Outra vantagem dessa técnica é que as dimensões originais são, de certa forma, \"combinadas\", o que resulta em uma nova forma de representar a combinação de termos. No contexto de PLN, essa técnica é conhecida como <b>Latent Semantic Analysis (LSA)</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25fe4a4a7f1b74b4835c3d270612e533ffef9d20"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_transformer = TruncatedSVD(n_components=1000)\n",
    "\n",
    "svd_transformer.fit(tfs)\n",
    "\n",
    "print(sorted(svd_transformer.explained_variance_ratio_)[::-1][:30])#lista de variância dos componentes, os 30 primeiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c1f5922d00b2ba009a2be6138222c55fd48fd06"
   },
   "source": [
    "<p>Agora vamos manter as dimensões até que a variância acumulada seja maior ou igual a 0.50.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5887fbc34a108d6af7522f44c36b26087d1c090d"
   },
   "outputs": [],
   "source": [
    "cummulative_variance = 0.0\n",
    "k = 0\n",
    "for var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n",
    "    cummulative_variance += var\n",
    "    if cummulative_variance >= 0.5:\n",
    "        break\n",
    "    else:\n",
    "        k += 1\n",
    "        \n",
    "print(k)#143 colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d5ec027d93824e292e763918cd6c2b419315b34"
   },
   "source": [
    "<p>Transformarmos novamente, mas desta vez com o número de k componentes que obtemos anteriormente.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b97f42f08b4cf665647883836e392ae65627786"
   },
   "outputs": [],
   "source": [
    "svd_transformer = TruncatedSVD(n_components=k)\n",
    "svd_data = svd_transformer.fit_transform(tfs)\n",
    "print(sorted(svd_transformer.explained_variance_ratio_)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0269f8c3eb95595a7c56cfe6211f70c849c80332"
   },
   "outputs": [],
   "source": [
    "print(svd_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc989f8172e55bde24f72daa5cbe67c1998dec43"
   },
   "source": [
    "<h2> Similaridade de Cosseno</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec435da433c7fcc0dd52bd295370c6c454acb1e3"
   },
   "source": [
    "<p>Uma vez que dois documentos são representados como vetores numéricos, é possível comparar a similaridade entre os documentos ao calcular o cosseno do ângulo entre esses documentos - uma medida de distância, não de amplitude. Para isso, basta resolver a equação do produto escalar entre os vetores, para encontrar o cosseno.</p>\n",
    "\n",
    "<img src=\"http://s0.wp.com/latex.php?latex=++%5Cdisplaystyle++%5Cvec%7Ba%7D+%5Ccdot+%5Cvec%7Bb%7D+%3D+%5C%7C%5Cvec%7Ba%7D%5C%7C%5C%7C%5Cvec%7Bb%7D%5C%7C%5Ccos%7B%5Ctheta%7D+%5C%5C+%5C%5C++%5Ccos%7B%5Ctheta%7D+%3D+%5Cfrac%7B%5Cvec%7Ba%7D+%5Ccdot+%5Cvec%7Bb%7D%7D%7B%5C%7C%5Cvec%7Ba%7D%5C%7C%5C%7C%5Cvec%7Bb%7D%5C%7C%7D++&amp;bg=ffffff&amp;fg=000000&amp;s=0\" alt=\"  \\displaystyle  \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}  \" title=\"  \\displaystyle  \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}  \" class=\"latex\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdf38b4d45935b61f2d3c75b3460623ffd64a490"
   },
   "source": [
    "<img class=\" wp-image-2582 \" title=\"vector_space\" src=\"http://blog.christianperone.com/wp-content/uploads/2013/09/vector_space.png\" alt=\"\" width=\"504\" height=\"378\">\n",
    "<p fontsize=8>Fonte: http://blog.christianperone.com</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be6d306294f78eb9ccedda02351df5298c45761c"
   },
   "outputs": [],
   "source": [
    "    documents = (\n",
    "    \"The sky is blue\",\n",
    "    \"The sun is bright\",\n",
    "    \"The sun in the sky is bright\",\n",
    "    \"We can see the shining sun, the bright sun\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d31152584e3abd32b862fcc961c911d3e59122d8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41abb374235f50b25623af7cd9284ed7167a45d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_vect = tfidf_vectorizer.transform([\"The sun is red in the sky\"])#ele segmenta e tokeniza automático, mas é bom criar o próprio\n",
    "\n",
    "cosine_similarity(query_vect, tfidf_matrix)#próx de um é mais similar, é a terceira frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "desired_width = 320\n",
    "pd.set_option('display.width', desired_width)\n",
    "np.set_printoptions(linewidth=desired_width)\n",
    "pd.set_option('display.max_columns',20)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "944cb088e94e6fefe9ada5fd2983496a873a9f39"
   },
   "source": [
    "<p><b>Exercício 3:</b>Escreva um chatbot que, dado uma pergunta em Inglês, encontre uma pergunta mais parecida no corpus de perguntas e respostas disponível no Kaggle (https://www.kaggle.com/rtatman/questionanswer-dataset#S08_question_answer_pairs.txt) e exiba a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/S08_question_answer_pairs.txt', sep='\\t')\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df.Question)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexSimilarity(question):\n",
    "    query_vect = tfidf_vectorizer.transform([question])\n",
    "    similarity = cosine_similarity(query_vect, tfidf_matrix)\n",
    "    i = np.where(similarity[0] == similarity.max())\n",
    "    return i[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qst = input('The question is: ')\n",
    "#qst = Who killed lincoln? #Did Lincoln beat in John\n",
    "index = getIndexSimilarity(qst)\n",
    "print('The most similar question was: {} \\nThe answer is: {}'.format(df.Question.iloc[index],df.Answer.iloc[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via Hardcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qst = \"Who killed Lincoln?\"\n",
    "index = getIndexSimilarity(qst)\n",
    "print('The most similar question was: {} \\nThe answer is: {}'.format(df.Question.iloc[index],df.Answer.iloc[index]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
