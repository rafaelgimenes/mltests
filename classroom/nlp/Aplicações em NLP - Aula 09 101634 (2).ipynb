{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n<h2 align=\"center\"> Aula 09 - Geração Automática de Texto </h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>"},{"metadata":{},"cell_type":"markdown","source":"<h2> 1. Introdução</h2>\n\n<p>Enquanto na Extração de Informação a tarefa principal estava relacionada em compreender a escrita e tratar ambiguidades ou similaridades no texto, na geração automática de texto o desafio está em decidir como expressar uma dada informação em palavras.\n    </p>\n    \n  <p>Há diversas aplicações práticas para NLG (do inglês, Natural Language Generation), como:</p>\n  \n  * Sumarização de documentos;\n  * Geração de notícias;\n  * Geração de texto para chatbots.\n  \n  <p>Nesta aula vamos abordar algumas técnicas para geração automática de texto, usando redes neurais recorrentes.</p>"},{"metadata":{},"cell_type":"markdown","source":"<h2>2. Técnicas para Geração de Texto</h2>\n\n<p> O objetivo da técnica de geração automática de texto é, dada uma sequência de n-gramas (ou caracteres, ou sentenças), prever qual será o próximo n-grama (ou caractere, ou sentença).\n    </p>"},{"metadata":{},"cell_type":"markdown","source":"<h2>3. Redes Neurais Recorrentes</h2>\n\n<p>Uma rede neural recorrente é uma rede com um \"loop\", ou seja, uma rede que aproveita o aprendizado de uma amostra anterior para amostras seguintes. A figura abaixo, retirada de http://colah.github.io/posts/2015-08-Understanding-LSTMs/ ilustra sua arquitetura.\n    </p>\n    \n![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)    \n\n<p>Perceba que essa arquitetura de redes neurais é especialmente aplicável para problemas com sequências, uma vez que as amostras anteriores influenciam na aprendizagem.</p>\n\n<p>Porém, as redes recorrentes convencionais tem limitações para utilizar informações aprendidas em etapas mais distantes (que acabam sendo atualizadas durante o treino). As redes neurais LSTM vieram para tentar sanar esse problema, como veremos à seguir.</p>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Redes LSTM</h3>\n\n<p>As redes LSTM (Long-Short Term Memory, ou de Memória de curto e longo prazo, numa tradução livre), ao contrário das redes neurais recorrentes convencionais, que apenas atualizam o estado aprendido em etapas anteriores, trazem o conceito de memória, que é controlada pelo modelo.\n    </p>\n    <p>Desta forma, esse modelo não apenas armazena os estados aprendidos em etapas anteriores, mas decide o que deve ser aproveitado, descartado ou atualizado. Vejamos a ilustração abaixo, também extraída de http://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n    </p>\n    \n![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)    \n\n<p>As etapas ilustradas são: </p>\n1. *Qual estado de memória deve ser lembrado*: A primeira camada usa uma função sigmoid que retorna um valor entre 0 e 1 para decidir se o estado de memória vindo da iteração anterior deve ser mantido (1 para completamente mantido) ou esquecido (0 para completamente esquecido);\n2. *Qual estado de memória deve ser atualizado*: Primeiramente usa-se uma função sigmoid para decidir quais valores serão atualizados com os novos dados da amostra atual, depois usa-se uma função tanh (que retorna um valor entre -1 e 1) para criar um novo vetor de estados da memória(denominado Ct) que poderá ser utilizado na iteração seguinte;\n3. *Atualizar as células de memória antigas*: Combina os estados que devem ser lembrados, os estados que devem ser atualizados e os novos estados para atualizar as células de memória, que serão utilizadas na iteração seguinte;\n4. *Decide a saída*: Por fim, a saída da iteração é feita através da combinação de uma função sigmoid e uma função tanh, considerando o estado atual da memória.\n\n    "},{"metadata":{},"cell_type":"markdown","source":"<h2>4. Usando LSTM para geração de texto</h2>\n\n<p> Vamos nos basear no exemplo disponível em https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms, e vamos usar um LSTM para geração de texto.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Activation, Dense, Dropout\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.core import Dense, Activation\nimport keras.utils as kutils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import nltk\n#nltk.corpus.gutenberg.fileids()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#raw_txt = nltk.corpus.gutenberg.raw(\"shakespeare-hamlet.txt\") + \\\n#nltk.corpus.gutenberg.raw(\"shakespeare-caesar.txt\") + \\\n#nltk.corpus.gutenberg.raw(\"shakespeare-macbeth.txt\")\n\n# Código adaptado de https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\nimport pandas as pd\nimport os\n\ncurr_dir = '../input/'\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\n\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nimport string\n\nall_sents = [[w.lower() for w in word_tokenize(sen) if not w in string.punctuation] \\\n             for sen in all_headlines]\n\nx = []\ny = []\n\nprint(all_sents[:10])\n\nfor sen in all_sents:\n    for i in range(1, len(sen)):\n        x.append(sen[:i])\n        y.append(sen[i])\n        \n\nprint(x[:10])\nprint(y[:10])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\n\nall_text = [c for sen in x for c in sen]\nall_text += [c for c in y]\n\nall_text.append('UNK') # Palavra desconhecida\n\nwords = list(set(all_text))\n        \nword_indexes = {word: index for index, word in enumerate(words)}      \n\nmax_features = len(word_indexes)\n\nx = [[word_indexes[c] for c in sen] for sen in x]\ny = [word_indexes[c] for c in y]\n\nprint(x[:10])\nprint(y[:10])\n\ny = kutils.to_categorical(y, num_classes=max_features)\n\nmaxlen = max([len(sen) for sen in x])\n\nprint(maxlen)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = pad_sequences(x, maxlen=maxlen)\nx = pad_sequences(x, maxlen=maxlen)\n\nprint(x[:10,-10:])\nprint(y[:10,-10:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x[:10,-10:])\n\nfor y_ in y:\n    for i in range(len(y_)):\n        if y_[i] != 0:\n            print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos criar nosso modelo de LSTM usando o Keras.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 10\n\nmodel = Sequential()\n    \n# Add Input Embedding Layer\nmodel.add(Embedding(max_features, embedding_size, input_length=maxlen))\n    \n# Add Hidden Layer 1 - LSTM Layer\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.1))\n    \n# Add Output Layer\nmodel.add(Dense(max_features, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x, y, epochs=20, verbose=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nprint(\"Saving model...\")\nmodel.save('shak-nlg.h5')\n\nwith open('shak-nlg-dict.pkl', 'wb') as handle:\n    pickle.dump(word_indexes, handle)\n\nwith open('shak-nlg-maxlen.pkl', 'wb') as handle:\n    pickle.dump(maxlen, handle)\nprint(\"Model Saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nmodel = keras.models.load_model('shak-nlg.h5')\nmaxlen = pickle.load(open('shak-nlg-maxlen.pkl', 'rb'))\nword_indexes = pickle.load(open('shak-nlg-dict.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vejamos como o algoritmo prevê uma palavra simples.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_seed = input()\nsample_seed_vect = np.array([[word_indexes[c] if c in word_indexes.keys() else word_indexes['UNK'] \\\n                    for c in word_tokenize(sample_seed)]])\n\nprint(sample_seed_vect)\n\nsample_seed_vect = pad_sequences(sample_seed_vect, maxlen=maxlen)\n\nprint(sample_seed_vect)\n\npredicted = model.predict_classes(sample_seed_vect, verbose=0)\n\nprint(predicted)\n\ndef get_word_by_index(index, word_indexes):\n    for w, i in word_indexes.items():\n        if index == i:\n            return w\n        \n    return None\n\n\nfor p in predicted:    \n    print(get_word_by_index(p, word_indexes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><b>Exercício 9</b>: Escreva um algoritmo que gera palavras até 100 palavras, com base em um texto curto de entrada (no máximo 5 palavras).</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_seed = input()\nsample_seed_vect = np.array([[word_indexes[c] if c in word_indexes.keys() else word_indexes['UNK'] \\\n                    for c in word_tokenize(sample_seed)]])\n\nprint(sample_seed_vect)\n\npredicted=[]\n\nsample_seed_vect = pad_sequences(sample_seed_vect, maxlen=maxlen)\n\nprint(sample_seed_vect)\n\nwhile (len(sample_seed_vect)<100):\n    predicted = model.predict_classes(pad_sequences([sample_seed_vect],maxlen=maxlen,padding=pre), verbose=0)\n    sample_seed_vect.extend(predicated)\n    print(predicted)\n\nres = []\nfor w in sample_seed_vect:\n    res.append(get_word_by_index(w,word_indexs))\n    \nprint(' '.join(res))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}